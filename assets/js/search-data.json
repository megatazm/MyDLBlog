{
  
    
        "post0": {
            "title": "Pengenalan Kepada Perceptron",
            "content": "Assalamualaikum. Selamat berjumpa kembali. . Seperti yang saya maklumkan pada artikel yang lepas, saya telah berjanji untuk cuba memberikan sedikit penerangan asas berkenaan teori DL ini. Kalau nak diikutkan, jika kita menggunakan library Tensorflow atau Pytorch nanti, kesemua yang kita bincang ini akan diabstrakkan implementasinya. . Dengan kata lain, &quot;Tak payah tau pun, pakai jer&quot;. . Tapi saya rasa anda perlu tahu juga konsep asasnya terlebih dahulu, sebab ini akan membantu anda untuk majukan ke tahap penggunaan DL yang lebih kompleks. Lagipun, tak kanlah nak pakai jer library Tensorflow atau Pytorch membuta tuli kan? . Perceptron . Perceptron adalah salah satu architecture ANN yang paling asas, dicipta pada tahun 1957 oleh Frank Rosenblatt. Ia berdasarkan pada konsep neuron (lihat gambar di bawah) yang disebut Treshold Logic Unit (TLU), ataupun Logic Treshold Unit (LTU). Nilai input dan outputnya adalah nombor (bukan nilai on/off binari), dan setiap sambungan di antara input nod dan output nod itu diberikan nilai pemberat atau weight. Secara analoginya, lagi besar weight maka lagi tebal lah wayar sambungannya (Lagi besar pengaruhnya). . . TLU menghitung jumlah weight berserta dengan inputnya seperti yang ditunjukkan oleh rumus di bawah, . kemudian hasil z itu diaplikasikan kepada fungsi step(z) untuk menghasilkan nilai output yang baharu, . Menyingkap kembali pada artikel yang sebelum ini, ia masih menggunakan rumus yang sama iaitu y = ax + b tanpa pembolehubah b. Merujuk pada ini, ada sedikit perbezaan iaitu nilai output adalah hasil dari 3 input fitur x dengan nilai pengaruh weight w (dahulunya a) pada setiap fitur tersebut. Perbezaan yang paling utama adalah peranan Step Function sebagai &quot;Activation Function&quot;. Idea activation function ini penting untuk membolehkan kita membina model yang non-linear (perceptron adalah algoritma linear untuk klasifikasi binari kerana nilai aktivasinya adalah 1 atau 0). Kenapa kita perlukan model non-linear? Cuba ingat balik fakta dari artikel yang lepas, garisan linear hanya mampu menghasilkan suatu model yang underfit atau high bias. Ini bukan suatu model yang kita inginkan untuk menyelesaikan masalah yang kompleks. . Mungkin ada yang tertanya-tanya kenapa ada huruf superscript T di atas huruf x itu? . . Ini kerana ia melibatkan pengiraan menggunakan Matrix Multiplication. Janganlah rasa takut bila dengar terma matrix ini. Anda hanya perlu tahu asas berkenaan ilmu matematik matrix yang di pelajari di sekolah menengah. Lagipun semua pengiraan ini nanti akan dilakukan oleh library Tensorflow atau Pytorch. . Ok. Huruf T adalah merujuk kepada proses matrix yang di panggil Transpose. Ia diperlukan untuk membetulkan bentuk dimensi matrix x bagi membolehkan ia multiply dengan matrix w. Contoh, jika bentuk dimensi matrix x dan w adalah (3,1), operasi matrix dimensi x(3,1) x w(3,1) itu perlu ditukarkan ke bentuk x(1,3) x w(3,1) terlebih dahulu dengan melakukan proses transpose pada matrix x. . Jika anda sudah lupa berkenaan operasi asas matrix, bolehlah rujuk kembali pada buku SPM anda. InsyaAllah, sayang anda pada cikgu matematik akan bertambah. . Ok. Bagaimana pula dengan Step Function? . Cuba lihat notasi matematik di bawah. Ia merujuk kepada dua jenis fungsi Step bernama Heaviside dan Sign. Dulu saya cukup fobia bila melihat notasi matematik dengan simbol macam cacing ni. Tapi sebenarnya jika anda cuba amati notasi tersebut ia sebenarnya adalah merupakan satu &quot;bahasa&quot; simbolik. Jika anda hafal maksud simbol-simbolnya maka mudahlah memahami maksudnya. . Fungsi heaviside akan mengeluarkan nilai 0 jika nilai z adalah lebih kecil daripada 0. Sebaliknya, ia mengeluarkan nilai 1 jika nilai z adalah sama atau lebih besar daripada 0. . . . . Manakala fungsi Sign pula akan mengeluarkan nilai -1 jika nilai z adalah lebih kecil daripada 0. Ia juga mengeluarkan nilai 0 jika nilai z adalah 0, dan mengeluarkan nilai +1 jika nilai z adalah lebih besar daripada 0. . . . Untuk memberi gambaran jelas pada yang &quot;visual learner&quot;, cuba lakarkan graf pada paksi z -&gt; X dan sgn(z) -&gt; Y. Saya serahkan pada anda untuk melakarkannya. . Architecture TLU tunggal ini boleh digunakan untuk klasifikasi binari yang mudah seperti Hujan (1) atau Tidak (0). Ia menghitung kombinasi linear dari 3 input x (dengan nilai weight masing-masing), dan jika hasil z melebihi nilai treshold (z &gt;= 0) seperti yang ditetapkan oleh contohnya fungsi heaviside, ia akan menghasilkan prediksi kelas positif (1). Jika sebaliknya, ia menghasilkan prediksi kelas negatif (0). Melatih TLU dalam kes ini bermaksud mencari nilai yang optimum untuk weight w1, w2, dan w3 itu. . Perceptron hanya terdiri daripada satu lapisan TLU, dengan setiap TLU disambungkan ke semua input. Apabila semua neuron dalam lapisan disambungkan ke setiap neuron pada lapisan sebelumnya (iaitu, neuron inputnya), lapisan tersebut disebut fully connected layer, atau dense layer. Input fitur yang memasuki lapisan TLU itu dianggap sebagai lapisan input neuron ( tiada pemprosesan berlaku di neuron ini). Kebiasaannya, input bias di tambah (seperti nilai b pada y=ax+b untuk linear regression model bergerak ke atas dan bawah paksi y, tetapi untuk DL ada tujuan tambahan) menggunakan neuron khas yang disebut neuron bias (Ia ditetapkan dengan nilai 1). Gambarajah di bawah menunjukkan perceptron dengan dua input, satu input bias dan tiga output. . . Apa tujuan lain bias? kenapa nilainya disetkan dengan nilai 1? . Anda tahu bahawa nilai output untuk 1 neuron TLU dan 1 input ditentukan oleh input, weight dan activation function step heaviside seperti berikut, . y = f (x₀ × w₀) . Sekiranya input adalah x₀ = 0 maka y = f (0) = 1. . Ini menyebabkan neutron tersebut sentiasa menghasilkan output yang sama (1) walaupun nilai weight w₀ berubah-ubah. Ia menyebabkan neuron itu berhenti belajar (neuron tidak aktif). Nilai bias=1 (y = f (x₀ × w₀ + 1) ), memastikan supaya neuron itu masih aktif walaupun nilai inputnya 0 (bias mengubah takat treshold atau trigger value activation function untuk neuron tersebut). . Semestinya nilai bias=1 sahaja ker? Tidak! . Bias juga adalah learnable parameter seperti weight parameter. Nilai 1 itu adalah tetapan permulaannya sahaja. Merujuk kepada gambarajah perceptron di atas, rumus untuk menghitung output lapisan fully connected adalah seperti berikut. . Penerangan rumus: . X mewakili matrix fitur input. Ia mempunyai satu row pada setiap rekod (juga di panggil sample atau instance) dan satu column pada setiap fitur (Awak bayangkan row dan column pada MS Excel). Merujuk pada gambarajah perceptron di atas, jika kita ada 1000 instance, maka saiz matrik X adalah (1000, 2). | Matrix weight W mengandungi semua sambungan weight antara neuron input dan neuron output kecuali neuron bias. Ia mempunyai satu row bagi setiap neuron input dan satu column bagi setiap neuron TLU lapisan output. Merujuk pada gambarajah di atas (kita ada 2 neuron input dan 3 neuron output), maka saiz matrix W adalah (2,3). | Vektor bias b mengandungi semua hubungan weight antara neuron bias dan neuron ouput. Iaitu ia mempunyai satu neuron bias bagi setiap neuron output TLU. | Fungsi φ ialah fungsi activation. TLU menggunakan fungsi step sebagai fungsi activationnya (kita akan membincangkan fungsi activation lain pada artikel yang seterusnya). | . Bagaimanakah proses training dilakukan oleh perceptron? . Konsepnya adalah sama dengan yang telah saya jelaskan pada artikel yang sebelum ini. Seperti contoh, setiap data instance atau sample itu dilabelkan dengan nilai target seperti contoh, Hujan(1) &amp; Tidak(0), Dalam proses training, hanya satu instance atau sample di sumbat masuk pada satu masa. Namun hanya input fitur x (tanpa nilai target) sahaja yang akan disambungkan pada neuron TLU. Manakala nilai target daripada instance itu akan digunakan untuk mengukur error (nilai ramalan - target label). Output TLU neuron menghasilkan (melalui fungsi step) nilai ramalan sama ada 1 atau 0 berdasarkan pada variasi nilai weight w. Pada setiap kali (melalui proses iteration) neuron output itu menghasilkan ramalan yang salah, ia akan mempertingkatkan nilai weight w antara neuron input dan TLU tersebut sehingga ramalannya menjadi betul. Iaitu pada setiap iteration, nilai weight w akan diupdatekan (lihat rumus di bawah. i adalah row, j adalah column) mengikut error sama ada ke arah 1 atau 0. Tetapi macam mana nak dikaitkan nilai dengan nilai error (ramalan - target label)? . Secara logiknya adalah dengan mengurangkan weight w jika neuron diaktifkan (1) walaupun ia sepatutnya tidak (0) diaktifkan, (ramalan: y = 1 dan target: t = 0). . Sebaliknya dengan meningkatkan weight w jika neuron tidak (0) diaktifkan walaupun seharusnya diaktifkan (1), (ramalan: y = 0 dan target: t = 1). . Oleh itu kita boleh gantikan dengan . Iaitu, jika sepatutnya aktif (1): . Δw(i,j)=-(0–1)=1 (tingkatkan weight w) . dan jika tidak sepatutnya aktif (0): . Δw(i,j)=-(1–0)=-1 (kurangkan weight w) . Kita masih lagi ada satu masalah, bagaimana kalau nilai input X adalah negative? Ini akan menyebabkan nilai output neuron menjadi terbalik . output TLU = h(-XW + b) . Keadaan ini akan menyebabkan neuron tidak diaktifkan walaupun sepatutnya ia perlu diaktifkan (t=1, y=0). Begitu juga sebaliknya (t=0, y=1). Oleh itu rumus ini diperbaiki menjadi seperti berikut, . di mana: . w(i,j) adalah weight yang menghubungkan input i ke output neuron j | y(j) adalah output ramalan | t(j) adalah nilai target label | x(i) adalah input fitur | µ adalah kadar kelajuan pembelajaran (learning rate). | . Apa keperluannya untuk set nilai learning rate? Tujuannya adalah untuk memperlahankan sedikit kadar kelajuan neuron itu belajar. . Aiik! kenapa nak diperlahankan pulak? Bukan lagi cepat lagi bagus ker? Tidak! . Macam ni lah. Ia ibarat macam anda nak memasukkan benang ke dalam lubang di jarum. Cucuk masuk cepat-cepat bagus ke? Masih ingat di artikel sebelum ni? Learning rate juga adalah salah satu hyperparameter tuning yang anda perlu lakukan. . Di bawah adalah contoh mudah penggunaan perceptron. Anda perlulah mempunyai ilmu asas dalam programming untuk memahami kod python itu. . Sebelum itu. Mungkin ada yang tak pasti macam mana nak setup environment untuk menjalankan aktiviti ini? Ikut langkah di bawah . Pastikan anda ada account google (gmail, gdrive) | Login ke Google Colab | Klik New Notebook | Copy-paste kod di bawah | Klik kod pada cell notebook | Klik simbol start di bahagian atas kiri cell | Sila lihat screenshot seperti di bawah. . . Apa yang anda perlu buat dengan kod ini? Anda cuma perlu tekan butang run sahaja. Kemudian cubalah baca komen-komen di &quot;#&quot;, yang telah saya coretkan pada kod tersebut untuk memahami fungsinya. Cuba main-main dengan kod itu. Jangan takut, tukarlah apa-apa sahaja yang tercetus di hati anda. . &quot;Belajar melalui pengalaman, Belajar melalui kesilapan&quot; . . import numpy as np # 1. Kita guna IRIS data download daripada sklearn library # Apa itu IRIS Data:-&gt; https://en.wikipedia.org/wiki/Iris_flower_data_set # 2. Kita guna Perceptron daripada library sklearn (kita tak guna KERAS Tensorflow atau Pytorch buat masa ini) from sklearn.datasets import load_iris from sklearn.linear_model import Perceptron # 3. Kita guna jer function load_iris untuk download IRIS data ke variable iris iris = load_iris() # 4. Uncomment ni kalau nak paparkan iris data. Iris data ada 4 input fitur #print(iris.data) # 5. Uncomment ni kalau nak paparkan iris target label. Iris data ada 3 target label klas (0,1,2) # 0-&gt;Setosa, 1-&gt;Versicolor, 2-&gt;Virginica : (perceptron hanya boleh klasifikasi 2 klas sahaja) #print(iris.target) # 6. Untuk contoh ini, kita cuma guna 2 input fitur sahaja daripada iris data # &quot;:&quot; bermaksud ambil semua row, (2,3) bermaksud ambil fitur column element ke 3 dan 4 sahaja (ingat array index bermula dari 0) # Iaitu fitur: 3-&gt;petal length, 4-&gt;petal width X = iris.data[:, (2, 3)] # Tukarkan klas 0-&gt;1, 1 dan 2-&gt;0 (Tukar kepada Klas Binary (1,0) untuk kegunaan Perceptron) # 1-&gt;Setosa atau 0-&gt;Bukan Setosa y = (iris.target == 0).astype(np.int) # Iris setosa? #7. Uncomment untuk lihat hasil penukaran data #print(X) #print(y) #Cipta Perceptron Neuron per_clf = Perceptron() #Train Model Perceptron dengan Data input X dan Data Target Label y per_clf.fit(X, y) #Gunakan model untuk membuat ramalan klasifikasi menggunakan Test Data [5.1,1.8] y_pred = per_clf.predict([[5.1, 1.8]]) #Paparkan Hasil Prediksi (1-&gt;Setosa atau 0-&gt;Bukan Setosa) print(y_pred) . [0] . Ok. Saya rasa kita berhenti di sini dahulu. Untuk artikel seterusnya, saya akan cuba untuk mengimplementasikan satu contoh lengkap penggunaan perceptron bagi tujuan klasifikasi binari. . Assalamualaikum dan InsyAllah kita bertemu lagi. .",
            "url": "https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/29/basic-dl.html",
            "relUrl": "/deep%20learning/2020/12/29/basic-dl.html",
            "date": " • Dec 29, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Pengenalan Kepada AI, ML & DL",
            "content": "Salam semua. Kita selalu mendengar perkataan “Artificial Intelligent” atau “AI”. Kalau masa awal-awal dulu kita rasa kembang hidung bila dapat mengguna perkataan ini dalam perbualan atau pun ceramah kita. Tapi, adakah setakat tahu menyebutnya sahaja sudah cukup? Tidak kah teringin untuk mengetahuinya dengan lebih mendalam? Ya, saya mahu! Anda bagaimana? Mari kita lakukan sedikit kajian. Maklumat di hujung jari anda! . Selain daripada AI, mungkin juga anda pernah dengar terma Machine Learning (ML) atau Deep Learning (DL) kan. Sejak kebelakangan ini kerap sangat kita dengar tiga perkataan ini sinonim dengan terma Data Science lah Data Analytic lah kan? Di sini saya nak bincangkan berkenaan tiga terma yang popular ini. Apa bendanya AI, ML, dan DL ni? Ahh zaman sekarang ni apa pun google jer. . . Ohh! DL adalah subset ML dan subset AI. AI dalam bahasa melayunya “kecerdasan buatan” boleh dicapai melalui pelbagai teknik. ML adalah salah satu tekniknya yang popular di masa kini. Manakala DL adalah salah satu teknik algoritma ML yang sedang hangat diperkatakan. Selain DL ada banyak lagi algoritma-algoritma lain yang popular seperti SVM, XGBoost, Random Forest yang sangat bagus untuk data yang tersusun dalam bentuk tabular data atau pun database. DL pula sangat berkuasa untuk menyelesaikan isu berkaitan “perceptual problem” seperti data yang berkaitan dengan imej, video, suara, dan bahasa. . Secara teorinya semakin banyak input data yang berkualiti dibekalkan, semakin baik prestasi outputnya. Namun, Gambarajah di bawah menunjukkan, prestasi algoritma tradisional seperti SVM, Random Forest, XGBoost akan mencapai takat tepu mendatar walaupun bekalan data yang di sumbat masuk semakin banyak. Sebaliknya, prestasi NN pula di lihat boleh ditingkatkan sejajar dengan jumlah input data yang semakin banyak. . . Ehh sekejap, ini algoritma NN (Neural Network), bukan DL (Deep Learning)! . Pada yang tidak tahu, DL ini adalah nama branding terkini untuk NN yang di pelopori oleh Bapa Deep Learning, Geoffry Hinton. Apa tu Neural Network? mungkin anda tertanya-tanya kan. Takpa itu letak di tepi dulu, cuba lihat gambarajah di bawah. . . Gambarajah architecture NN ini terdiri daripada 3 lapisan, iaitu:- . Lapisan Pertama, Input Layer beserta 3 node atau feature | Lapisan Kedua, Hidden Layer beserta 4 node atau feature | Lapisan Ketiga, 1 Output node. | . Ini merupakan contoh architecture asas NN yang sekarangnya dipanggil Shallow NN. Pada masa dahulu lebih kurang pada tahun 1950 - 1960, architecture Shallow NN sahaja lah yang mampu di capai, namun kemudiannya idea NN ini terkubur berdekad-dekad lamanya (kita akan bincangkan sejarah DL kemudian). Kini, architecture NN boleh di bina dengan lebih Deep, jumlah node dan lapisan yang lebih banyak dan kompleks. Secara teorinya semakin banyak node dan lapisan architecturenya maka semakin tinggi lah kerumitan masalah yang ia boleh selesaikan, tetapi semakin tinggi pula kuasa pemprosesan yang diperlukan. . Ok. Sebelum kita menjebakkan diri ke dalam dunia DL, kenali ML terlebih dahulu. . Machine Learning . Machine learning jika diterjemahkan secara terus kepada bahasa melayu apa maksudnya? Mesin Belajar? Belajar Mesin? atau mungkin lebih tepat jika ia bermaksud mesin yang boleh di ajar. Mesin yang satu hari nanti boleh mengajar &quot;diri&quot;nya sendiri mungkin kedengaran menakutkan ataupun kelakar, tetapi menurut Elon Musk, jika penggunaan teknologi DL ini tidak di kawal-selia dengan baik ia mungkin bakal menjadi lebih bahaya daripada kepala bom nuklear! . Ok, kenapa ML adalah teknik AI yang sangat hebat di waktu ini. . Sebagai contoh, katakan kita nak mengajar komputer untuk mengenali haiwan yang dikenali oleh manusia sebagai kucing. Mungkin agak mudah mengajar bayi yang berumur 3 tahun untuk mengenalinya, tetapi bolehkah anda bayangkan bagaimana sukar untuk melakukannya melalui teknik traditional AI? Secara traditionalnya, untuk memberi arahan atau mengajar komputer membuat perbandingan, ia boleh dilaksanakan dengan pendekatan rule-based melalui programming ataupun pengaturcaraan. Seperti contoh, dengan berpandukan pada pixel-pixel gambar seekor kucing seperti di bawah, anda perlu memprogram ribuan kod rule-based yang kompleks untuk membolehkan komputer mengenali kucing tersebut. . . Bisa lakukannya? Ya mungkin boleh, berkat ketekunan dan kesabaran anda. Alhamdulillah selesai masalah. . Tunggu! Tunggu sebentar! Masih awal untuk bergumbira. . Cuba lihat gambar-gambar kucing di bawah. Bagaimana pula dengan kucing-kucing lain? ada ratus ribuan kucing di luar sana malahan berpuluh-puluh spesis kucing wujud di dunia ini. Bagaimana pula dengan keadaan kucing-kucing itu? ada yang sedang mengendap di tepi bakul, di urut tengkoknya, mulut mengiau luas, mendukung anaknya, dan sebagainya. . . Dengan kata lain, bagaimana kita nak &quot;generalize&quot; kan kod rule-based ini supaya ia dapat mengenali kucing-kucing yang belum di &quot;lihat&quot; oleh komputer. Disinilah kekuatan ML mengatasi teknik traditional AI. Lihat gambarajah di bawah. Kaedah traditional programming, seperti yang dinyatakan di atas, ia memerlukan manusia membina program berdasarkan pada input data untuk menghasilkan model klasifikasi rule-based. Manakala, ML tidak memerlukan manusia untuk membina program tersebut secara eksplisit. Apa yang ML perlu ialah bantuan manusia melabelkan jawapan betul pada setiap input data, dan menyalurkankannya ke dalam komputer. Seterusnya, algoritma ML memproses data tersebut lalu membina model yang boleh mengklasifikasi data input yang belum di &quot;lihat&quot; sebagai kucing atau bukan kucing. Oleh kerana model klasifikasi tersebut boleh di bina secara automatik dan pantas oleh algoritma ML (dengan sedikit bantuan daripada manusia), kita boleh meningkatkan prestasi &quot;generalization&quot; model tersebut dengan menyumbat masuk pelbagai jenis gambar kucing kepada algoritma ML itu dengan skala yang besar (lebih banyak input data yang pelbagai, lebih tinggi prestasi generalization sebuah model). . . Secara informalnya, ML boleh didefinasikan sebagai:- . Teknik AI yang membolehkan machine berupaya belajar tanpa perlu di program secara eksplisit Secara formalnya pula ia didefinasikan sebagai:- . Suatu program komputer dikatakan belajar dari pengalaman E terhadap suatu kelas tugasan T dengan prestasinya di ukur dengan P. Prestasi pada ukuran P terhadap tugasan T meningkat melalui penambahan pengalaman E. Definasi E, T &amp; P dengan frasa lebih mudah:- . T = Tugasan untuk mengenali kucing di dalam gambar | E = Pengalaman melihat banyak gambar-gambar kucing | P = Kebarangkalian program mengenal pasti kucing di dalam gambar | . Untuk pengetahuan anda, ML terdiri dari beberapa kategori; iaitu Supervised Learning, Unsupervised Learning, Semi-Supervised Learning, Self-Supervised Learning, Reinforcement Learning, dan banyak lagi. Cuma kali ini, kita fokus pada teknik yang saya bincangkan di atas iaitu, Supervised Learning. Supervised Learning sangat mustajab digunakan untuk menyelesaikan masalah berkaitan Classification dan Regression (Saya akan jelaskan hal berkenaan regression kemudian). . Secara analoginya, Supervised Learning Classification adalah ibarat mengajar kanak-kanak mengenal kucing menggunakan kad flip. . Cikgu: Ini apa? | Murid: Itu ayam. | Cikgu: Bukan, Ini kucing. | Murid: Itu kucing. | . Supervised Learning . Seperti yang dinyatakan sebelum ini, idea supervised learning classification adalah berdasarkan pada konsep mengajar kanak-kanak (murid) menggunakan kad flip. . Cikgu = Supervisor | Murid = Komputer | Input Data = Gambar Kucing | Input Label = Kapsyen pada Kad Flip | . Gambarajah di bawah menunjukkan aliran proses supervised learning. . . Langkah pertama untuk menjalankan proses Supervised Learning (SL) adalah mengumpul dan melabel atau menganotasi data. Kita bernasib baik sebab gambar-gambar kucing sudah pun ada banyak di google.com, mudah lah untuk kita mencuba ML dan DL nanti. Kita bukan sahaja boleh mendapatkan data daripada google.com dan Google Dataset Search, ada banyak benchmark data yang telah digunakan oleh penyelidik-penyelidik, ada di internet. Senarai dataset tersebut boleh di perolehi di sini, TowardsAI dan Wikipedia. . Ini adalah suatu kemudahan kan? banyak data sudah sedia ada untuk memberi galakan kepada anda menerokai teknologi ini (Anda masih perlu melakukan sedikit proses penyesuaian data, ianya agak mudah dilakukan). . Data Preprocessing . Namun, jika anda ingin mengunakan ML untuk menyelesaikan permasalahan sebenar di luar sana, pengumpulan dan penyediaan datanya tidak lah semudah itu. Anda perlu melalui proses Data Pre-processing terlebih dahulu. . Data Cleaning Missing Data Contohnya, jika ada atribut rekod di dalam data tersebut hilang, padamkan rekod itu. | Ataupun simpan rekod itu, tetapi nilai pada atribut yang hilang digantikan dengan nilai average atribut. (Rekod dan atribut adalah satu jaluran informasi dalam data) | . | Jika data tersebut adalah imej, kita boleh membuang data yang tidak sesuai secara visual sebelum atau secara automatik selepas training. Fastai adalah framework deep learning untuk Pytorch, mempunyai fitur untuk menyenaraikan data yang tidak baik secara automatik selepas menjalankan proses training. | Noisy Data Noisy data adalah data yang dikatakan &quot;meaningless&quot;, &quot;takda maknanya&quot;. Macam bunyi noise + muzik, bunyi noise itu kalau tak ada lagi sedap bunyi muziknya. Ia boleh diuruskan melalui teknik seperti Binning, Regression, dan Clustering (melandaikan data, mengurangkan zig-zag). Ada juga yang mencadangkan dengan menambahkan lebih banyak data yang berkualiti untuk mengurangkan kesan noisy data. Kaedah-kaedah lain adalah seperti teknik dimension reduction (seperti algoritma PCA), teknik Regularization, dan Cross Validation. Ironinya noise (random noise) merupakan rakan baik untuk algoritma DL. Contohnya, random noise sengaja di suntik masuk ke dalam data (teknik Data Augmentation) sebelum ia disalurkan ke dalam algoritma DL regression untuk meningkatkan tahap generalization modelnya (salah satu teknik Regularization dalam DL). Manakala algoritma DL Generative Adversarial Network (GAN) pula menggunakan random noise sebagai dasar untuk mencipta &quot;fake face images&quot;, muka manusia tiruan. | . | Outlier Data Outlier data adalah data yang berada jauh daripada kelompok data lain dari segi kesamaan dan keberkaitannya. Kebiasaannya outlier data perlu di buang (mungkin disebabkan human error), namun ada ketikanya seperti untuk anomaly detection (mengesan sesuatu di luar kebiasaan seperti aktiviti hacker, network intrusion, dan penipuan dalam talian), dalam konteks ini outlier data adalah penting. | . | . | Data Transformation Normalization Normalization menyeragamkan skala nilai pada setiap atribut di rekod. Kebanyakkan algoritma memerlukan semua atribut tersebut diseragamkan di antara julat yang sama. Contohnya, jika julat atribut Harga Rumah adalah 50,000 dan 500,000 dan julat atribut Jumlah Bilik adalah 2 dan 8 (Contoh atribut data berkenaan hartanah), ia perlu diseragamkan di dalam julat yang sama untuk membolehkan algoritma berfungsi dengan baik. Algoritma seperti NN sebagai contoh, memerlukan input data yang diseragamkan kepada nilai di antara 0 dan 1. | . | Discretization Menukarkan julat nombor tertentu kepada perkataan. Contohnya, nilai antara 1 dan 17 -&gt; Kanak-Kanak, 18 dan 39 -&gt; Dewasa, 40 dan 59 -&gt; Pertengahan, 60 dan ke atas -&gt; Wargamas. | . | Encoding Categorical Value mengekod nilai atribut dari perkataan kepada nombor binary. Contoh, Kucing -&gt; 100, Ayam -&gt; 010, dan Monyet -&gt; 001. Ini perlu dilakukan terutamanya untuk algoritma seperti NN yang hanya boleh berfungsi dengan nilai input, outputnya di dalam bentuk nombor sahaja. | . | Data Labeling Data labeling atau annotation yang diperkatakan di atas juga adalah teknik data preprocessing yang mesti dilakukan sebelum proses supervised learning. Jika data tersebut adalah imej, anda perlu menanda pada imej tersebut, objek yang ingin diklasifikasikan. Bayangkan jika anda perlu menganotasikan pada 50 ribu keping gambar yang mana setiap keping gambar itu ada 1000 orang! | . | . | Data Dimension Reduction Feature Extraction Memilih atribut-atribut yang relevan dan menggabungkannya menjadi satu atribut yang baharu. | . | Feature Selection Memilih atribut yang relevan dan membuang atribut yang tidak relevan. | . | Kedua-dua teknik ini bukan sahaja mengurangkan dimensi data, ia juga dikatakan akan menjadikan data tersebut lebih bermakna dan mudah di latih oleh algoritma ML. Ada banyak algoritma yang boleh digunakan untuk data dimension reduction ini, tapi saya tak membincangkannya di sini kerana algoritma DL yang kita akan terokai nanti dikatakan mampu beraksi dengan baik tanpa perlu melalui proses ini dilakukan secara manual. | . | . Model Training . Apakah yang dimaksudkan dengan model training? . Graf di bawah menunjukkan hubungan antara dua pemboleh ubah X, Y dan garisan persamaan linear di garis melintasi data yang diplotkan. Proses ini dikenali sebagai Simple Linear Regression Model, contoh ini adalah yang paling mudah kita gunakan kerana ia hanya melibatkan dua pemboleh ubah (atribut 2 dimensi). Bulatan kuning adalah training data manakala garisan biru adalah model yang digariskan oleh algoritma ML melalui proses training. Anak panah berwarna merah merupakan jarak atau error antara training data dan nilai pada garisan model f(X)-&gt;Y. Melalui proses training, algoritma ML perlu membina garisan model f(X)-&gt;Y yang minima jumlah errornya berbanding dengan point training data (seperti yang ditunjukkan pada graf sebelah kanan). . . Rumus di bawah adalah persamaan linear yang merujuk pada garisan yang berwarna biru. Nilai pemboleh ubah a menentukan kecerunan, manakala pemboleh ubah b menentukan ketinggian garisan tersebut pada paksi Y. Graf di sebelah kiri merupakan garisan permulaan model linear regression itu pada ketika a=0 dan b=1. Pada ketika ini nilai prediksi model f(X) -&gt; Y pada setiap titik x training data adalah 1. Jumlah error ketika ini boleh di hitung dengan menggunakan rumus Mean Square Error (MSE). . . Merujuk pada rumus MSE di atas: . x adalah independant variable ataupun input fitur yang digunakan oleh model untuk melakukan prediksi nilai dependant variable ataupun target value, y. | prediction(x) adalah nilai prediksi y di input x berdasarkan model linear regression ketika di suatu nilai a dan b. | D adalah training data yang telah dilabelkan. x ialah input fitur manakala y ialah output labelnya. Contoh graf di atas menunjukkan ada 6 data point dengan input fitur dan label seperti berikut:- (x1,y1), (x2,y2), (x3,y3), (x4,y4), (x5,y5), (x6,y6) . | N adalah jumlah training data (contoh di atas, N = 6). | . Untuk menghitung MSE, pada setiap data point, dapatkan error antara nilai y label dan y prediksi, kemudian kuasa duakannya. Lakukan ini pada setiap data point, kemudian jumlahkan kesemuanya. Akhir sekali, bahagikan hasil jumlah tersebut dengan jumlah training data, N (merujuk pada contoh di atas, N = 6). . Proses yang merujuk pada graf di kiri itu adalah dikatakan sebagai &quot;initial iteration per 1 epoch&quot;. . Apa maksudnya? . Initial di sini bermaksud nilai permulaan parameter a dan b bagi mengariskan kedudukan awal garisan model linear tersebut. Nilai a dan b itu boleh ditentukan supaya bermula dengan nilai kosong ataupun nilai random (Untuk architecture NN yang kompleks kita mungkin perlu menggunakan kaedah lain yang lebih spesifik). . Iteration adalah frekuensi algoritma ML itu disuapi sebahagian daripada training data. Epoch pula adalah frequency kesemua training data itu selesai di proses oleh algoritma ML tersebut. . Boleh faham tak maksudnya? Ok, saya jelaskan dengan lebih terperinci. . Dalam keadaan sebenar, kesemua training data itu tidak di suap masuk ke dalam algoritma secara serentak. Ia di suap mengikut kumpulan atau batch. Contoh, jika jumlah training data adalah 2000 (1 epoch = 2000 data di proses). Jika anda setkan saiz batch = 100, maka setiap 1 epoch = 20 iteration (20 x 100 = 2000). Jika anda setkan training proses sebanyak 250 epoch maka total iterationnya adalah 20 x 250 = 5000. Ingat, pengiraan MSE berlaku pada setiap iteration. Macam mana, Ok? . Untuk contoh di atas, oleh kerana training datanya cuma ada 6 sahaja, maka setiap 1 epoch ada 1 iteration sahaja (kita setkan saiz batch = jumlah training data, iaitu 6). Oleh itu pada setiap 1 iteration, nilai MSE di hitung berdasarkan pada kedudukan terkini garisan linear model. Kedudukan garisan linear model akan berubah pada setiap iteration. Dengan harapan nilai MSEnya semakin kecil. . Apa yang akan berlaku jika anda setkan training proses sebanyak 250 epoch? Cukup ker? . Maksudnya, algoritma ML tersebut ada masa sebanyak 250 iteration untuk mengariskan linear regression modelnya (melalui kombinasi nilai parameter a dan b) supaya MSE mencapai nilai seminima yang mungkin. Secara teorinya, lagi lama iteration maka semakin tinggilah peluangnya mencapai nilai MSE yang paling minima. Namun ini bergantung kepada keupayaan algoritma ML menentukan nilai parameter a dan b yang optimum dalam jangkamasa yang singkat. Bayangkan jika algoritma ini menggunakan teknik random atau brute-force bagi menentukan nilai parameter a dan b itu. Iteration sebanyak 250 mungkin tidak mencukupi kan? . Algoritma Deep Learning menggunakan teknik optimization yang di panggil &quot;Gradient Decent&quot; untuk mencapai nilai MSE yang minima, jauh lebih pantas berbanding teknik secara random. . Bayangkan jikalau bukan 2, tapi ada beribu parameter yang perlu ditentukan? Dengan Gradient Decent, ia mungkin memakan masa dalam beberapa puluh minit, tapi jika secara random, anda mungkin perlu menunggunya selama beribu-ribu tahun! . . Gambarajah di atas adalah contoh linear regression model yang mencapai tahap optimum pada iteration yang ke-28 berdasarkan pada jumlah training datanya. . Model Validation . Saya harap anda sudah memperolehi kefahaman asas berkenaan model training, tapi mungkin ada yang tertanya-tanya, apa kaitannya validation data dengan model training? Saya pun langsung tak melibatkan validation data di dalam penerangan berkenaan model training tadi. . Jika anda merujuk kembali pada gambarajah aliran proses supervised learning. Ada tiga jenis data yang perlu disediakan selepas proses labeling data, . Training Data | Validation Data | Testing Data | . Seperti yang telah saya jelaskan sebelum ini, training data diperlukan untuk membina model classification ataupun regression. Manakala, testing data pula adalah &quot;unseen data&quot; yang telah diasingkan bagi tujuan mengukur prestasi model tersebut. . Sebenarnya fungsi validation data juga adalah sama dengan test data iaitu mengukur prestasi model yang telah di bina. Cuma bezanya adalah, validation data digunakan untuk mengukur prestasi model ketika di dalam proses model training. . Ada beberapa teknik validation iaitu, . Holdout Method Sebahagian dari training data diasingkan kepada validation data (mungkin dalam nisbah 90% training dan 10% validation data, atau 80% dan 20%). | . | K-Fold Cross Validation . Teknik ini sesuai digunakan untuk membolehkan anda mengoptimumkan penggunaan training data. Terutamanya, jika anda tidak mempunyai training data yang cukup banyak. . | Gambarajah di bawah menunjukkan gambaran proses 10-fold cross validation (K=10). . | Berikut adalah langkah-langkah proses 10-fold cross-validation, . Asingkan data kepada 10 bahagian yang sama atau &quot;folds&quot;. | Train model anda menggunakan 9 folds data yang pertama. | Buat penilaian prestasi model itu menggunakan baki ataupun &quot;hold-out&quot; data fold yang ke-10 tadi. | Ulangi langkah (2) dan (3) sebanyak 10 kali, di mana setiap ulangan menggunakan hold-out data fold yang berbeza. | Hitungkan purata prestasi model daripada kesemua 10 hold-out data folds tersebut. | | . | Stratified K-Fold Cross Validation . Teknik ini masih menggunakan langkah yang sama seperti di atas. Stratified adalah nama teknik sampling data yang digunakan untuk memilih data ketika mengasingkan data kepada training dan validation. Teknik sampling data yang digunakan oleh dua teknik validation sebelum ini adalah random data sampling. Teknik stratified data sampling digunakan jika data yang anda perolehi itu tidak seimbang. Contohnya, training data untuk harga rumah yang mahal lebih banyak daripada harga rumah yang murah, ataupun training data gambar kucing ada lebih banyak daripada gambar anjing dan monyet. | . | . K-fold cross validation dikatakan, . Secara signifikan mengurangkan &quot;bias&quot; kerana menggunakan sebahagian besar training data yang ada. Ia juga secara signifikannya mengurangi &quot;variance&quot; kerana sebahagian besar data juga digunakan sebagai validation data. . Bias? Variance? mungkin ada yang tertanya-tanya akan maksudnya. . Istilah Bias &amp; Variance ini berkait rapat dengan keperluan untuk mengukur prestasi model ketika dalam proses model training yang kemudiannya dijadikan sebagai rujukan bagi melakukan proses Model Tuning. . Model Tuning &amp; Testing . Berkenaan dengan model tuning, berikut adalah beberapa istilah penting yang perlu anda fahami. . Bias &amp; Variance | Underfitting &amp; Overfitting | Generalization | Hyperparameter Tuning, Regularization &amp; Optimization . | Wow! Banyaknya nak kena hafal. . Jangan risau, tak banyak mana pun sebenarnya, sebab mereka ni semua saling berkaitan. Yang penting kita jelas dengan maksudnya supaya tak terkeliru dan dikelirukan. Ini kerana, di luar sana ada ramai yang mengamalkan teknik ini, . &quot;If you can&#39;t convince them, confused them&quot; . Untuk memudahkan anda faham, gambarajah di bawah menggarap maksud istilah Bias &amp; Variance, Underfitting &amp; Overfitting, dan Generalization sekali gus. . . Gambarajah di atas menunjukkan error rate menurun apabila kompleksiti model meningkat (Peningkatan kompleksiti model adalah berkadar terus dengan frequency iteration atau epoch). Iaitu, lagi lama proses model training berjalan maka lagi kompleks lah model yang terhasil. Secara teorinya, semakin lama model di train maka semakin kecil lah errornya (menghampiri kosong), namun ini tidak semestinya baik. . Kenapa pulak? Error model dah zero, mesti lah terbaik kan? Tidak! . Cuba anda lihat tiga graf kecil di bawah gambarajah itu. . Overfitting = High Variance, Underfitting = High Bias, Good Balance = Low Bias, Low Variance. . Apabila training iteration tak mencukupi, model yang dihasilkan adalah simple iaitu garisan linear yang tidak mampu untuk &quot;fit&quot; pada trend data yang non-linear. Sebaliknya, jika terlalu lama di train, model yang terlampau &quot;fit&quot; pada trend data itu akan terhasil. Cuba anda perhatikan, apabila semua garisan model itu &quot;overfit&quot; kerana ia melalui kesemua titik training data. Oleh itu, errornya memanglah menjadi kosong kan? (masih ingat pengiraan MSE?). Namun, prestasi model ini hanya baik pada data yang pernah dilihatnya sahaja dan bukan pada &quot;unseen&quot; data. . Inilah yang dikatakan sebagai &quot;Bias &amp; Variance Tradeoff&quot;. Iaitu, mencari Good Balance, Sweet Spot ataupun Generalized Model, mampu memberi prestasi yang baik apabila modelnya di nilai dengan data baharu. Oleh itu, dengan berpandukan pada titik error rate terendah validation dan test error, maka anda boleh menentukan nilai iteration atau epoch yang paling optimum. . Ok. Sekarang baru lah saya boleh bercakap tentang Model Tuning &amp; Testing. . Jika sudah selesai melakukan model training dengan tetapan nilai epoch yang optimum, maka anda telah bersedia untuk mengukur prestasi model anda dengan unseen test data (anda boleh melakukannya pada beberapa model terpilih). Jika anda bernasib baik, nilai ketepatannya mungkin lebih baik daripada validation error, atau mungkin sebaliknya. . Jangan cepat berpuas hati! Masih ada ruang untuk &quot;tune up&quot; prestasi model anda melalui Hyperparameter Tuning, Regularization &amp; Optimization (dalam konteks deep learning). . Hyperparameter adalah parameter pada algoritma yang boleh di ubah-ubah settingnya untuk mencari kombinasi yang optimum. Ibarat macam amplifier audio, kita pusing-pusing tombol-tombolnya untuk mencari bunyi yang paling enak. Hyperparameter juga dikatakan sebagai &quot;non-learnable&quot; parameter manakala &quot;learnable&quot; parameter hanyalah pada model parameter. Contoh model parameter ialah nilai parameter a dan b untuk simple linear regression model y = Ax + b. Ini adalah kerana nilai a dan b itu ditentukan (learned) oleh algoritma dan training data ketika proses model training. Regularization adalah teknik atau setting yang boleh meningkatkan tahap generalization model. Optimization adalah pemilihan jenis algoritma Gradient Decent (GD) dan setting parameternya. . Senarai hyperparameter pada DL algoritma adalah seperti berikut:- . Jumlah NN Hidden Layer (NN Parameter) | Jumlah Nod di setiap NN layer (NN Parameter) | Pemilihan Activation Function di setiap NN layer seperti RELU, Tanh, Sigmoid, etc (NN parameter) | Batch Normalization (Regularization) | Setting Learning Rate (GD Optimization) | Early stopping (Regularization) | L2 &amp; L1 norm (Regularizatioan) | Tambah training data (Regularization) | NN node Drop-out (Regularization) | Data Augmentation (Regularization) | Mini-batch size (GD Optimization) | Momentum (GD Optimization) | RMSprop (GD Optimization) | ADAM (GD Optimization) | Learning Rate Decay (GD Optimization) | . Banyak kan? Sebenarnya ada banyak lagi! 눈_눈 . Ini kalau nak diceritakan teori dan matematik merujuk pada semua di atas ni, Mau pecah la kepala otak saya. Tapi jangan risau, kita guna jer kod implementasinya di library Tensorflow/Keras dan Pytorch. Tapi kalau nak tune semua ni satu persatu, penat laa! lecehnya!. Ya betul memang leceh, tapi nasib baik ada library automation untuk tujuan hyperparameter tuning ni. Antara yang popular untuk Tensorflow/Keras dan Pytorch ialah, . AutoML | Keras Tuner | T-PoT | Hyperas | Talos | Hypersearch | Optuna | Determined | Ray Tune | . Banyak kan? Sebenarnya ada banyak lagi! 눈_눈 . Saya rasa cukuplah sampai di sini. Di artikel yang seterusnya InsyaAllah saya akan cuba menulis yang lebih spesifik pada teori berkaitan deep learning dan implementasinya pula. . Dengan ini, harapan saya semoga artikel ini dapat memberi manafaat kepada pembaca yang berminat meneroka di dalam teknologi ini. Jika anda ingin mengetahui dengan lebih lanjut, memanglah ada berlambak-lambak bahan-bahan pembelajaran berkenaan DL di google.com dan youtube. Tapi hampir kesemuanya adalah dalam bahasa inggeris lah, dan adalah juga sedikit dalam bahasa Indonesia. Mungkin tak menjadi masalah sangat pada rata-rata orang kita, kerana ramai orang Malaysia mahir berbahasa inggeris, pokoknya adalah minat dan kesabaran yang mendalam. . Assalamualaikum! .",
            "url": "https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/23/AI-ML-DL.html",
            "relUrl": "/deep%20learning/2020/12/23/AI-ML-DL.html",
            "date": " • Dec 23, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Artikel Lampau Berkaitan IoT",
            "content": "Sila klik di sini untuk pergi ke artikel yang dicoretkan sebelum ini. . Terima kasih. .",
            "url": "https://megatazm.github.io/MyDLBlog/iot/2020/12/17/previous-iot-article.html",
            "relUrl": "/iot/2020/12/17/previous-iot-article.html",
            "date": " • Dec 17, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that . . cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://megatazm.github.io/MyDLBlog/fastpages/2020/02/20/test.html",
            "relUrl": "/fastpages/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Testing Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://megatazm.github.io/MyDLBlog/fastpages/2020/01/14/test-markdown-post.html",
            "relUrl": "/fastpages/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Saya adalah seorang pensyarah di UniKL MIIT. Bidang teknologi yang saya minati ialah Deep Learning, Cloud Computing, and IoT. Tujuan saya menulis artikel di sini adalah dengan harapan untuk memberi galakan dan pendedahan awal pada pelajar-pelajar sekolah menengah khasnya, juga pada semua pelajar-pelajar universiti yang mungkin berminat untuk menelaah bahan pembelajaran berkaitan teknologi ini di dalam Bahasa Melayu. Saya harap blog ini sedikit sebanyak dapat memberi manafaat kepada anda semua. .",
          "url": "https://megatazm.github.io/MyDLBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://megatazm.github.io/MyDLBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}