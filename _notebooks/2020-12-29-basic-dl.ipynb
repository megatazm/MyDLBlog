{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2020-12-29-basic-dl.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP0K2oSMXDBXs7frj+1o3a6"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ReYmjuFcpi_v"},"source":["Assalamualaikum. Selamat berjumpa kembali.\n","\n","Seperti yang saya maklumkan pada [artikel yang lepas](https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/23/AI-ML-DL.html), saya telah berjanji untuk cuba memberikan sedikit penerangan asas berkenaan teori DL ini. Kalau nak diikutkan, jika kita menggunakan library Tensorflow atau Pytorch nanti, kesemua yang kita bincang ini akan diabstrakkan implementasinya. \n","\n","Dengan kata lain, \"Tak payah tau pun, pakai jer\". \n","\n","Tapi saya rasa anda perlu tahu juga konsep asasnya terlebih dahulu, sebab ini akan membantu anda untuk majukan ke tahap penggunaan DL yang lebih kompleks. Lagipun, tak kanlah nak pakai jer library Tensorflow atau Pytorch membuta tuli kan?"]},{"cell_type":"markdown","metadata":{"id":"sz_uQONCoQW7"},"source":["# Pengenalan Kepada Deep Learning\n","> Perbincangan berkenaan konsep asas deep learning\n","\n","- toc: true \n","- badges: false\n","- comments: true\n","- categories: [deep learning]\n","- image: images/\n","- author: Megat Norulazmi"]},{"cell_type":"markdown","metadata":{"id":"BfobCvuKM1fT"},"source":["# Perceptron\n","***Perceptron*** adalah salah satu *architecture* ***ANN***   yang paling asas, dicipta pada tahun 1957 oleh Frank Rosenblatt. Ia berdasarkan pada konsep *neuron* (lihat gambar di bawah) yang disebut ***Treshold Logic Unit*** (TLU), ataupun ***Logic Treshold Unit*** (LTU). Nilai input dan outputnya adalah nombor (bukan nilai on/off binari), dan setiap sambungan di antara input nod dan output nod itu diberikan nilai pemberat atau ***weight***. Secara analoginya, lagi besar *weight* maka lagi tebal lah wayar sambungannya (Lagi besar pengaruhnya). "]},{"cell_type":"markdown","metadata":{"id":"Da_Z9aQwUfCH"},"source":["<br/>\n","<br/>\n","<img src=\"fastcore_imgs/perceptron.png\" width=\"80%\" height=\"80%\" alt=\"perceptron\">\n","<br/>\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"GSv_9xoWCCmG"},"source":["TLU menghitung jumlah *weight* berserta dengan inputnya seperti yang ditunjukkan oleh rumus di bawah,\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20z%3D%20w_%7B1%7Dx_%7B1%7D%20+%20w_%7B2%7Dx_%7B2%7D%20+..+%20w_%7Bn%7Dx_%7Bn%7D%20%3D%20x%5E%7BT%7Dw)\n","<br/>\n","\n","kemudian hasil ***z*** itu diaplikasikan kepada fungsi ***step(z)*** untuk menghasilkan nilai output yang baharu,\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20h_%7Bw%7D%28x%29%20%3D%20step%28z%29%2C%20where%20%5C%20z%20%3D%20x%5E%7BT%7Dw)\n","<br/>\n","\n","Menyingkap kembali pada artikel yang [sebelum ini](https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/23/AI-ML-DL.html), ia masih menggunakan rumus yang sama iaitu ***y = ax + b*** tanpa pembolehubah ***b***. Merujuk pada ini, ada sedikit perbezaan iaitu nilai output adalah hasil dari 3 input fitur ***x*** dengan nilai pengaruh weight ***w*** (dahulunya ***a***) pada setiap fitur tersebut. Perbezaan yang paling utama adalah peranan ***Step Function*** sebagai \"***Activation Function***\". Idea *activation function* ini penting untuk membolehkan kita membina model yang ***non-linear*** (perceptron adalah algoritma linear untuk klasifikasi binari kerana nilai aktivasinya adalah 1 atau 0). Kenapa kita perlukan model *non-linear*? Cuba ingat balik fakta dari artikel yang lepas, garisan ***linear*** hanya mampu menghasilkan suatu model yang ***underfit*** atau ***high bias***. Ini bukan suatu model yang kita inginkan untuk menyelesaikan masalah yang kompleks."]},{"cell_type":"markdown","metadata":{"id":"50cMmuivJAOD"},"source":["Mungkin ada yang tertanya-tanya kenapa ada huruf *superscript* ***T*** di atas huruf ***x*** itu?\n","<br/>\n","<br/>\n",">![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20x%5E%7BT%7Dw)\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"809U3ai_UYKh"},"source":["Ini kerana ia melibatkan pengiraan menggunakan *Matrix Multiplication*. Janganlah rasa takut bila dengar terma matrix ini. Anda hanya perlu tahu asas berkenaan ilmu matematik *matrix* yang di pelajari di sekolah menengah. Lagipun semua pengiraan ini nanti akan dilakukan oleh library Tensorflow atau Pytorch. \n","\n","Ok. Huruf ***T*** adalah merujuk kepada proses matrix yang di panggil ***Transpose***. Ia diperlukan untuk membetulkan bentuk dimensi matrix ***x*** bagi membolehkan ia multiply dengan matrix ***w***. Contoh, jika bentuk dimensi matrix ***x*** dan ***w*** adalah ***(3,1)***, operasi matrix dimensi ***x(3,1)*** x ***w(3,1)*** itu perlu ditukarkan ke bentuk ***x(1,3)*** x ***w(3,1)*** terlebih dahulu dengan melakukan proses transpose pada matrix ***x***.\n","\n","Jika anda sudah lupa berkenaan operasi asas matrix, bolehlah rujuk kembali pada buku SPM anda. InsyaAllah, sayang anda pada cikgu matematik akan bertambah. "]},{"cell_type":"markdown","metadata":{"id":"mllUUhOqI1S-"},"source":["Ok. Bagaimana pula dengan Step Function? \n","\n","Cuba lihat notasi matematik di bawah. Ia merujuk kepada dua jenis fungsi Step bernama ***Heaviside*** dan ***Sign***. Dulu saya cukup fobia bila melihat notasi matematik dengan simbol macam cacing ni. Tapi sebenarnya jika anda cuba amati notasi tersebut ia sebenarnya adalah merupakan satu \"bahasa\" simbolik. Jika anda hafal maksud simbol-simbolnya maka mudahlah memahami maksudnya.\n","\n","Fungsi ***heaviside*** akan mengeluarkan nilai ***0*** jika nilai ***z*** adalah lebih kecil daripada ***0***. Sebaliknya, ia mengeluarkan nilai ***1*** jika nilai ***z*** adalah sama atau lebih besar daripada ***0***.\n","\n","\n","\n","<br/>\n","<br/>\n","\n","![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20heaviside%28z%29%3D%5Cbegin%7Bcases%7D%20%26%20%5Ctext%7B0%20if%20%7D%20z%3C%200%20%5C%5C%20%26%20%5Ctext%7B1%20if%20%7D%20z%5Cgeq%200%20%5Cend%7Bcases%7D)\n","\n","<br/>\n","\n","Manakala fungsi ***Sign*** pula akan mengeluarkan nilai ***-1*** jika nilai ***z*** adalah lebih kecil daripada ***0***. Ia juga mengeluarkan nilai ***0*** jika nilai ***z*** adalah ***0***, dan mengeluarkan nilai ***+1*** jika nilai ***z*** adalah lebih besar daripada ***0***.\n","\n","<br/>\n","\n","![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20sgn%28z%29%3D%5Cbegin%7Bcases%7D%20%26%20%5Ctext%7B%20-1%20if%20%7D%20z%3C%200%20%5C%5C%20%26%20%5Ctext%7B%20%5C%200%20if%20%7D%20z%3D%200%20%5C%5C%20%26%20%5Ctext%7B+1%20if%20%7D%20z%3E%200%20%5Cend%7Bcases%7D)\n","<br/>\n","\n","Untuk memberi gambaran jelas pada yang \"***visual learner***\", cuba lakarkan graf pada paksi ***z -> X*** dan ***sgn(z) -> Y***. Saya serahkan pada anda untuk melakarkannya."]},{"cell_type":"markdown","metadata":{"id":"XOb0P28RuHjl"},"source":["*Architecture* **TLU** tunggal ini boleh digunakan untuk klasifikasi binari yang mudah seperti **Hujan (1)** atau **Tidak (0)**. Ia menghitung kombinasi linear dari 3 input ***x*** (dengan nilai weight masing-masing), dan jika hasil ***z*** melebihi nilai ***treshold*** (***z >= 0***) seperti yang ditetapkan oleh contohnya fungsi ***heaviside***, ia akan menghasilkan prediksi kelas **positif (1)**. Jika sebaliknya, ia menghasilkan prediksi kelas **negatif (0)**. Melatih **TLU** dalam kes ini bermaksud mencari nilai yang optimum untuk *weight* ***w1***, ***w2***, dan ***w3*** itu.\n","\n","Perceptron hanya terdiri daripada satu lapisan TLU, dengan setiap TLU disambungkan ke semua input. Apabila semua neuron dalam lapisan disambungkan ke setiap neuron pada lapisan sebelumnya (iaitu, neuron inputnya), lapisan tersebut disebut ***fully connected layer***, atau ***dense layer***. Input fitur yang memasuki lapisan TLU itu dianggap sebagai lapisan input neuron ( tiada pemprosesan berlaku di neuron ini). Kebiasaannya, input fitur ***bias*** (seperti nilai ***b*** pada ***y=ax+b*** untuk linear regression, tetapi tujuannya untuk DL berbeza) ditambahkan menggunakan neuron khas yang disebut **neuron bias** (Ia ditetapkan dengan nilai 1). Gambarajah di bawah menunjukkan perceptron dengan dua input dan tiga output.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Xd-mb3yHk3c3"},"source":["<br/>\n","<br/>\n","<img src=\"fastcore_imgs/perceptron-2i-3o-1b.png\" width=\"80%\" height=\"80%\" alt=\"perceptron-2i-3o-1b\">\n","<br/>\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"ieVpIWPHlS_0"},"source":["Apa keperluan bias disetkan dengan nilai 1?\n","\n","Anda tahu bahawa nilai output neuron ditentukan oleh input, weight dan activation function step seperti berikut,\n","\n","*** y = f (x₀ × w₀) ***\n"," \n","Sekiranya input adalah ***x₀ = 0*** maka ***y = f (0) = 1***.\n","\n","Ini menyebabkan neutron tersebut sentiasa menghasilkan output yang sama (1) walaupun nilai weight ***w₀*** berubah-ubah. Ia menyebabkan neuron itu berhenti belajar."]}]}