{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2020-12-29-basic-dl.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPBFLNkooD3Fdxrm29vySLO"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ReYmjuFcpi_v"},"source":["Assalamualaikum. Selamat berjumpa kembali.\n","\n","Seperti yang saya maklumkan pada [artikel yang lepas](https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/23/AI-ML-DL.html), saya telah berjanji untuk cuba memberikan sedikit penerangan asas berkenaan teori DL ini. Kalau nak diikutkan, jika kita menggunakan library Tensorflow atau Pytorch nanti, kesemua yang kita bincang ini akan diabstrakkan implementasinya. \n","\n","Dengan kata lain, \"Tak payah tau pun, pakai jer\". \n","\n","Tapi saya rasa anda perlu tahu juga konsep asasnya terlebih dahulu, sebab ini akan membantu anda untuk majukan ke tahap penggunaan DL yang lebih kompleks. Lagipun, tak kanlah nak pakai jer library Tensorflow atau Pytorch membuta tuli kan?"]},{"cell_type":"markdown","metadata":{"id":"sz_uQONCoQW7"},"source":["# Pengenalan Kepada Perceptron\n","> Perbincangan berkenaan konsep asas deep learning\n","\n","- toc: true \n","- badges: false\n","- comments: true\n","- categories: [deep learning]\n","- image: images/\n","- author: Megat Norulazmi"]},{"cell_type":"markdown","metadata":{"id":"BfobCvuKM1fT"},"source":["# Perceptron\n","***Perceptron*** adalah salah satu *architecture* ***ANN***   yang paling asas, dicipta pada tahun 1957 oleh Frank Rosenblatt. Ia berdasarkan pada konsep *neuron* (lihat gambar di bawah) yang disebut ***Treshold Logic Unit*** (TLU), ataupun ***Logic Treshold Unit*** (LTU). Nilai input dan outputnya adalah nombor (bukan nilai on/off binari), dan setiap sambungan di antara input nod dan output nod itu diberikan nilai pemberat atau ***weight***. Secara analoginya, lagi besar *weight* maka lagi tebal lah wayar sambungannya (Lagi besar pengaruhnya). "]},{"cell_type":"markdown","metadata":{"id":"Da_Z9aQwUfCH"},"source":["<br/>\n","<br/>\n","<img src=\"fastcore_imgs/perceptron.png\" width=\"80%\" height=\"80%\" alt=\"perceptron\">\n","<br/>\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"GSv_9xoWCCmG"},"source":["TLU menghitung jumlah *weight* berserta dengan inputnya seperti yang ditunjukkan oleh rumus di bawah,\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20z%3D%20w_%7B1%7Dx_%7B1%7D%20+%20w_%7B2%7Dx_%7B2%7D%20+..+%20w_%7Bn%7Dx_%7Bn%7D%20%3D%20x%5E%7BT%7Dw)\n","<br/>\n","\n","kemudian hasil ***z*** itu diaplikasikan kepada fungsi ***step(z)*** untuk menghasilkan nilai output yang baharu,\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20h_%7Bw%7D%28x%29%20%3D%20step%28z%29%2C%20where%20%5C%20z%20%3D%20x%5E%7BT%7Dw)\n","<br/>\n","\n","Menyingkap kembali pada artikel yang [sebelum ini](https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/23/AI-ML-DL.html), ia masih menggunakan rumus yang sama iaitu ***y = ax + b*** tanpa pembolehubah ***b***. Merujuk pada ini, ada sedikit perbezaan iaitu nilai output adalah hasil dari 3 input fitur ***x*** dengan nilai pengaruh weight ***w*** (dahulunya ***a***) pada setiap fitur tersebut. Perbezaan yang paling utama adalah peranan ***Step Function*** sebagai \"***Activation Function***\". Idea *activation function* ini penting untuk membolehkan kita membina model yang ***non-linear*** (perceptron adalah algoritma linear untuk klasifikasi binari kerana nilai aktivasinya adalah 1 atau 0). Kenapa kita perlukan model *non-linear*? Cuba ingat balik fakta dari artikel yang lepas, garisan ***linear*** hanya mampu menghasilkan suatu model yang ***underfit*** atau ***high bias***. Ini bukan suatu model yang kita inginkan untuk menyelesaikan masalah yang kompleks."]},{"cell_type":"markdown","metadata":{"id":"50cMmuivJAOD"},"source":["Mungkin ada yang tertanya-tanya kenapa ada huruf *superscript* ***T*** di atas huruf ***x*** itu?\n","<br/>\n","<br/>\n",">![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20x%5E%7BT%7Dw)\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"809U3ai_UYKh"},"source":["Ini kerana ia melibatkan pengiraan menggunakan *Matrix Multiplication*. Janganlah rasa takut bila dengar terma matrix ini. Anda hanya perlu tahu asas berkenaan ilmu matematik *matrix* yang di pelajari di sekolah menengah. Lagipun semua pengiraan ini nanti akan dilakukan oleh library Tensorflow atau Pytorch. \n","\n","Ok. Huruf ***T*** adalah merujuk kepada proses matrix yang di panggil ***Transpose***. Ia diperlukan untuk membetulkan bentuk dimensi matrix ***x*** bagi membolehkan ia multiply dengan matrix ***w***. Contoh, jika bentuk dimensi matrix ***x*** dan ***w*** adalah ***(3,1)***, operasi matrix dimensi ***x(3,1)*** x ***w(3,1)*** itu perlu ditukarkan ke bentuk ***x(1,3)*** x ***w(3,1)*** terlebih dahulu dengan melakukan proses transpose pada matrix ***x***.\n","\n","Jika anda sudah lupa berkenaan operasi asas matrix, bolehlah rujuk kembali pada buku SPM anda. InsyaAllah, sayang anda pada cikgu matematik akan bertambah. "]},{"cell_type":"markdown","metadata":{"id":"mllUUhOqI1S-"},"source":["Ok. Bagaimana pula dengan Step Function? \n","\n","Cuba lihat notasi matematik di bawah. Ia merujuk kepada dua jenis fungsi Step bernama ***Heaviside*** dan ***Sign***. Dulu saya cukup fobia bila melihat notasi matematik dengan simbol macam cacing ni. Tapi sebenarnya jika anda cuba amati notasi tersebut ia sebenarnya adalah merupakan satu \"bahasa\" simbolik. Jika anda hafal maksud simbol-simbolnya maka mudahlah memahami maksudnya.\n","\n","Fungsi ***heaviside*** akan mengeluarkan nilai ***0*** jika nilai ***z*** adalah lebih kecil daripada ***0***. Sebaliknya, ia mengeluarkan nilai ***1*** jika nilai ***z*** adalah sama atau lebih besar daripada ***0***.\n","\n","\n","\n","<br/>\n","<br/>\n","\n","![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20heaviside%28z%29%3D%5Cbegin%7Bcases%7D%20%26%20%5Ctext%7B0%20if%20%7D%20z%3C%200%20%5C%5C%20%26%20%5Ctext%7B1%20if%20%7D%20z%5Cgeq%200%20%5Cend%7Bcases%7D)\n","\n","<br/>\n","\n","Manakala fungsi ***Sign*** pula akan mengeluarkan nilai ***-1*** jika nilai ***z*** adalah lebih kecil daripada ***0***. Ia juga mengeluarkan nilai ***0*** jika nilai ***z*** adalah ***0***, dan mengeluarkan nilai ***+1*** jika nilai ***z*** adalah lebih besar daripada ***0***.\n","\n","<br/>\n","\n","![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20sgn%28z%29%3D%5Cbegin%7Bcases%7D%20%26%20%5Ctext%7B%20-1%20if%20%7D%20z%3C%200%20%5C%5C%20%26%20%5Ctext%7B%20%5C%200%20if%20%7D%20z%3D%200%20%5C%5C%20%26%20%5Ctext%7B+1%20if%20%7D%20z%3E%200%20%5Cend%7Bcases%7D)\n","<br/>\n","\n","Untuk memberi gambaran jelas pada yang \"***visual learner***\", cuba lakarkan graf pada paksi ***z -> X*** dan ***sgn(z) -> Y***. Saya serahkan pada anda untuk melakarkannya."]},{"cell_type":"markdown","metadata":{"id":"XOb0P28RuHjl"},"source":["*Architecture* **TLU** tunggal ini boleh digunakan untuk klasifikasi binari yang mudah seperti **Hujan (1)** atau **Tidak (0)**. Ia menghitung kombinasi linear dari 3 input ***x*** (dengan nilai weight masing-masing), dan jika hasil ***z*** melebihi nilai ***treshold*** (***z >= 0***) seperti yang ditetapkan oleh contohnya fungsi ***heaviside***, ia akan menghasilkan prediksi kelas **positif (1)**. Jika sebaliknya, ia menghasilkan prediksi kelas **negatif (0)**. Melatih **TLU** dalam kes ini bermaksud mencari nilai yang optimum untuk *weight* ***w1***, ***w2***, dan ***w3*** itu.\n","\n","Perceptron hanya terdiri daripada satu lapisan TLU, dengan setiap TLU disambungkan ke semua input. Apabila semua neuron dalam lapisan disambungkan ke setiap neuron pada lapisan sebelumnya (iaitu, neuron inputnya), lapisan tersebut disebut ***fully connected layer***, atau ***dense layer***. Input fitur yang memasuki lapisan TLU itu dianggap sebagai lapisan input neuron ( tiada pemprosesan berlaku di neuron ini). Kebiasaannya, input ***bias***  di tambah (seperti nilai ***b*** pada ***y=ax+b*** untuk linear regression model bergerak ke atas dan bawah paksi y, tetapi untuk DL ada tujuan tambahan) menggunakan neuron khas yang disebut **neuron bias** (Ia ditetapkan dengan nilai 1). Gambarajah di bawah menunjukkan perceptron dengan dua input, satu input bias dan tiga output.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Xd-mb3yHk3c3"},"source":["<br/>\n","<br/>\n","<img src=\"fastcore_imgs/perceptron-2i-3o-1b.png\" width=\"80%\" height=\"80%\" alt=\"perceptron-2i-3o-1b\">\n","<br/>\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"ieVpIWPHlS_0"},"source":["Apa tujuan lain bias? kenapa nilainya disetkan dengan nilai 1?\n","\n","Anda tahu bahawa nilai output untuk 1 neuron TLU  dan 1 input ditentukan oleh input, weight dan activation function step *heaviside* seperti berikut,\n","\n",">***y = f (x₀ × w₀)*** \n"," \n","Sekiranya input adalah ***x₀ = 0*** maka ***y = f (0) = 1***.\n","\n","Ini menyebabkan neutron tersebut sentiasa menghasilkan output yang sama (1) walaupun nilai weight ***w₀*** berubah-ubah. Ia menyebabkan neuron itu berhenti belajar (neuron tidak aktif). Nilai bias=1 (***y = f (x₀ × w₀ + 1)*** ), memastikan supaya neuron itu masih aktif walaupun nilai inputnya 0 (bias mengubah takat treshold atau trigger value activation function untuk neuron tersebut). \n","\n","Semestinya nilai bias=1 sahaja ker?  Tidak!\n","\n","Bias juga adalah *learnable* parameter seperti *weight* parameter. Nilai 1 itu adalah tetapan permulaannya sahaja. Merujuk kepada gambarajah perceptron di atas, rumus untuk menghitung output lapisan ***fully connected *** adalah seperti berikut.\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/png.download?%5Clarge%20h_%7Bw%2Cb%7D%5Cleft%20%28%20X%20%5Cright%20%29%3D%5Cphi%20%5Cleft%20%28%20XW+b%20%5Cright%20%29)\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"Fnmhdh71Hold"},"source":["Penerangan rumus:\n","\n","* ***X*** mewakili matrix fitur input. Ia mempunyai satu *row* pada setiap rekod (juga di panggil *sample* atau *instance*) dan satu column pada setiap fitur (Awak bayangkan row dan column pada MS Excel). Merujuk pada gambarajah perceptron di atas, jika kita ada 1000 instance, maka saiz matrik ***X*** adalah (1000, 2).\n","* Matrix weight ***W*** mengandungi semua sambungan weight antara neuron input dan neuron output kecuali neuron bias. Ia mempunyai satu row bagi setiap neuron input dan satu column bagi setiap neuron TLU lapisan output. Merujuk pada gambarajah di atas (kita ada 2 neuron input dan 3 neuron output), maka saiz matrix ***W*** adalah (2,3).\n","* Vektor bias ***b*** mengandungi semua hubungan weight antara neuron bias dan neuron ouput. Iaitu ia mempunyai satu neuron bias bagi setiap neuron output TLU.\n","* Fungsi φ ialah fungsi *activation*. TLU menggunakan fungsi step sebagai fungsi *activation*nya (kita akan membincangkan fungsi activation lain pada artikel yang seterusnya)."]},{"cell_type":"markdown","metadata":{"id":"8Ltu5cqwWF9E"},"source":["Bagaimanakah proses training dilakukan oleh perceptron?\n","\n","Konsepnya adalah sama dengan yang telah saya jelaskan pada artikel yang [sebelum ini](https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/23/AI-ML-DL.html). Seperti contoh, setiap data instance atau sample itu dilabelkan dengan nilai target seperti contoh, Hujan(1) & Tidak(0),\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/gif.download?%5Clarge%20%5C%7Bx_%7B1%7D%2Cx_%7B2%7D%2Ctarget%5C%7D%5Crightarrow%20%5C%7B32%2C25%2C1%5C%7D%2C%5C%7B31%2C24%2C1%5C%7D%5Ccdots%20%5C%7B50%2C45%2C0%5C%7D%2C%5C%7B51%2C44%2C0%5C%7D)\n","<br/>\n","Dalam proses training, hanya satu instance atau sample di sumbat masuk pada satu masa. Namun hanya input fitur ***x*** (tanpa nilai target) sahaja yang akan disambungkan pada neuron TLU. Manakala nilai target daripada instance itu akan digunakan untuk mengukur error (nilai ramalan - target label). Output TLU neuron menghasilkan (melalui fungsi step) nilai ramalan sama ada ***1*** atau ***0*** berdasarkan pada variasi nilai weight ***w***. Pada setiap kali (melalui proses iteration) neuron output itu menghasilkan ramalan yang salah, ia akan mempertingkatkan nilai weight ***w*** antara neuron input dan TLU tersebut sehingga ramalannya menjadi betul. Iaitu pada setiap iteration, nilai weight ***w*** akan diupdatekan (lihat rumus di bawah. ***i*** adalah ***row***, ***j*** adalah ***column***) mengikut error sama ada ke arah 1 atau 0.\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/gif.download?%5Clarge%20w_%7Bij%7D%3Dw_%7Bij%7D+%5CDelta%20w_%7Bij%7D)\n","<br/>\n","Tetapi macam mana nak dikaitkan nilai \n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/gif.download?%5Clarge%20%5CDelta%20w_%7Bij%7D)\n","<br/>\n","dengan nilai error (ramalan - target label)? \n","\n","Secara logiknya adalah untuk mengurangkan weight ***w*** jika neuron diaktifkan (1) walaupun ia sepatutnya tidak (0) diaktifkan, (ramalan: y = 1 dan target: t = 0). \n","\n","Sebaliknya dengan meningkatkan weight ***w*** jika neuron tidak (0) diaktifkan walaupun seharusnya diaktifkan (1), (ramalan: y = 0 dan target: t = 1).\n","\n","Oleh itu kita boleh gantikan dengan\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/gif.download?%5Clarge%20%5CDelta%20w_%7Bij%7D%3D-%28y-t%29)\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"bwk5yMDUjHHz"},"source":["Iaitu, jika sepatutnya aktif (1): \n","<br/>\n",">***Δw(i,j)=-(0–1)=1 (tingkatkan weight w)***\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"F8tK2coXjUXF"},"source":["dan jika tidak sepatutnya aktif (0):\n","<br/>\n",">***Δw(i,j)=-(1–0)=-1 (kurangkan weight w)***\n","<br/>\n","\n","Kita masih lagi ada satu masalah, bagaimana kalau nilai input X adalah negative? Ini akan menyebabkan nilai output neuron menjadi terbalik \n","\n",">***output TLU = h(-XW + b)***\n","\n","Keadaan ini akan menyebabkan neuron tidak diaktifkan walaupun sepatutnya ia perlu diaktifkan (t=1, y=0). Begitu juga sebaliknya (t=0, y=1). Oleh itu rumus ini diperbaiki menjadi seperti berikut,\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/png.download?%5Clarge%20%5CDelta%20w_%7Bij%7D%3Dw_%7Bij%7D-%5Cmu%20%28y_%7Bj%7D-t_%7Bj%7D%29%5Ctimes%20x_%7Bi%7D)\n","<br/>\n"]},{"cell_type":"markdown","metadata":{"id":"m3SMucH83fFJ"},"source":["di mana:\n","\n","* ***w(i,j)*** adalah *weight* yang menghubungkan input ***i*** ke output neuron ***j*** \n","* ***y(j)*** adalah output ramalan \n","* ***t(j)*** adalah nilai target label \n","* ***x(i)*** adalah input fitur\n","* ***µ*** adalah kadar kelajuan pembelajaran (*learning rate*).\n","\n","Apa keperluannya untuk set nilai *learning rate*? Tujuannya adalah untuk memperlahankan sedikit kadar kelajuan neuron itu belajar. \n","\n","Aiik! kenapa nak diperlahankan pulak? Bukan lagi cepat lagi bagus ker? Tidak!\n","\n","Macam ni lah. Ia ibarat macam anda nak memasukkan benang ke dalam lubang di jarum. Cucuk masuk cepat-cepat bagus ke? Masih ingat di artikel sebelum ni? *Learning rate* juga adalah salah satu *hyperparameter tuning* yang anda perlu lakukan.\n","\n","Di bawah adalah contoh mudah penggunaan perceptron. Anda perlulah mempunyai ilmu asas dalam programming untuk memahami kod python itu.\n","\n","Sebelum itu. Mungkin ada yang tak pasti macam mana nak setup environment untuk menjalankan aktiviti ini? Ikut langkah di bawah \n","\n","1.   Pastikan awak ada account google (gmail, gdrive)\n","2.   Login ke [Google Colab](https://colab.research.google.com/) \n","3.   Klik New Notebook\n","4.   Copy-paste kod di bawah\n","5.   Klik kod pada cell notebook\n","6.   Klik simbol start di bahagian atas kiri cell\n","\n","Sila lihat screenshot seperti di bawah.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RhqWYbKwEH4p"},"source":["<br/>\n","<br/>\n","<img src=\"fastcore_imgs/screenshot-googlecolab.png\" width=\"100%\" height=\"100%\" alt=\"screenshot-googlecolab\">\n","<br/>\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"6a6FPGF1ZZro"},"source":["Apa yang anda perlu buat dengan kod ini? Anda cuma perlu tekan butang run sahaja. Kemudian cubalah baca komen-komen di \"#\", yang telah saya coretkan pada kod tersebut untuk memahami fungsinya. Cuba main-main dengan kod itu. Jangan takut, tukarlah apa-apa sahaja yang tercetus di hati anda.\n","\n"," >\"Belajar melalui pengalaman, Belajar melalui kesilapan\"\n","\n","<br/>\n","<br/>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qaWrmIeo8Xd5","executionInfo":{"status":"ok","timestamp":1609998029821,"user_tz":-480,"elapsed":811,"user":{"displayName":"Azmi Megat","photoUrl":"","userId":"09952198438441952493"}},"outputId":"a17a099a-3eea-44e4-e861-c2b1526449e4"},"source":["import numpy as np\n","\n","# 1. Kita guna IRIS data download daripada sklearn library\n","# Apa itu IRIS Data:-> https://en.wikipedia.org/wiki/Iris_flower_data_set\n","# 2. Kita guna Perceptron daripada library sklearn (kita tak guna KERAS Tensorflow atau Pytorch buat masa ini)\n","from sklearn.datasets import load_iris        \n","from sklearn.linear_model import Perceptron    \n","\n","# 3. Kita guna jer function load_iris untuk download IRIS data ke variable iris\n","iris = load_iris()    \n","\n","# 4. Uncomment ni kalau nak paparkan iris data. Iris data ada 4 input fitur\n","#print(iris.data)\n","\n","# 5. Uncomment ni kalau nak paparkan iris target label. Iris data ada 3 target label klas (0,1,2)\n","#    0->Setosa, 1->Versicolor, 2->Virginica : (perceptron hanya boleh klasifikasi 2 klas sahaja) \n","#print(iris.target)\n","\n","# 6. Untuk contoh ini, kita cuma guna 2 input fitur sahaja daripada iris data\n","#    \":\" bermaksud ambil semua row, (2,3) bermaksud ambil fitur column element ke 3 dan 4 sahaja (ingat array index bermula dari 0)\n","#    Iaitu fitur: 3->petal length, 4->petal width \n","X = iris.data[:, (2, 3)]                       \n","\n","# Tukarkan klas 0->1, 1 dan 2->0 (Tukar kepada Klas Binary (1,0) untuk kegunaan Perceptron)\n","# 1->Setosa atau 0->Bukan Setosa\n","y = (iris.target == 0).astype(np.int) # Iris setosa?\n","\n","#7. Uncomment untuk lihat hasil penukaran data\n","#print(X)\n","#print(y)\n","\n","#Cipta Perceptron Neuron\n","per_clf = Perceptron()\n","\n","#Train Model Perceptron dengan Data input X dan Data Target Label y\n","per_clf.fit(X, y)\n","\n","#Gunakan model untuk membuat ramalan klasifikasi menggunakan Test Data [5.1,1.8]\n","y_pred = per_clf.predict([[5.1, 1.8]])\n","\n","#Paparkan Hasil Prediksi (1->Setosa atau 0->Bukan Setosa)\n","print(y_pred)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IZs4QYpFZ1_l"},"source":["<br/>\n","<br/>\n","Ok. Saya rasa kita berhenti di sini dahulu. Untuk artikel seterusnya, saya akan cuba untuk mengimplementasikan satu contoh lengkap penggunaan perceptron bagi tujuan klasifikasi binari. \n","\n","Assalamualaikum dan InsyAllah kita bertemu lagi."]}]}