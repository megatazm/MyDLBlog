{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2020-12-29-basic-dl.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP7TTBtazLVmLdM7M+Upmys"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ReYmjuFcpi_v"},"source":["Assalamualaikum. Selamat berjumpa kembali.\n","\n","Seperti yang saya maklumkan pada [artikel yang lepas](https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/23/AI-ML-DL.html), saya telah berjanji untuk cuba memberikan sedikit penerangan asas berkenaan teori DL ini. Kalau nak diikutkan, jika kita menggunakan library Tensorflow atau Pytorch nanti, kesemua yang kita bincang ini akan diabstrakkan implementasinya. \n","\n","Dengan kata lain, \"Tak payah tau pun, pakai jer\". \n","\n","Tapi saya rasa anda perlu tahu juga konsep asasnya terlebih dahulu, sebab ini akan membantu anda untuk majukan ke tahap penggunaan DL yang lebih kompleks. Lagipun, tak kanlah nak pakai jer library Tensorflow atau Pytorch membuta tuli kan?"]},{"cell_type":"markdown","metadata":{"id":"sz_uQONCoQW7"},"source":["# Pengenalan Kepada Perceptron\n","> Perbincangan berkenaan konsep asas deep learning\n","\n","- toc: true \n","- badges: false\n","- comments: true\n","- categories: [deep learning]\n","- image: images/\n","- author: Megat Norulazmi"]},{"cell_type":"markdown","metadata":{"id":"BfobCvuKM1fT"},"source":["# Perceptron\n","***Perceptron*** adalah salah satu *architecture* ***ANN***   yang paling asas, dicipta pada tahun 1957 oleh Frank Rosenblatt. Ia berdasarkan pada konsep *neuron* (lihat gambar di bawah) yang disebut ***Treshold Logic Unit*** (TLU), ataupun ***Logic Treshold Unit*** (LTU). Nilai input dan outputnya adalah nombor (bukan nilai on/off binari), dan setiap sambungan di antara input nod dan output nod itu diberikan nilai pemberat atau ***weight***. Secara analoginya, lagi besar *weight* maka lagi tebal lah wayar sambungannya (Lagi besar pengaruhnya). "]},{"cell_type":"markdown","metadata":{"id":"Da_Z9aQwUfCH"},"source":["<br/>\n","<br/>\n","<img src=\"fastcore_imgs/perceptron.png\" width=\"80%\" height=\"80%\" alt=\"perceptron\">\n","<br/>\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"GSv_9xoWCCmG"},"source":["TLU menghitung jumlah *weight* berserta dengan inputnya seperti yang ditunjukkan oleh rumus di bawah,\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20z%3D%20w_%7B1%7Dx_%7B1%7D%20+%20w_%7B2%7Dx_%7B2%7D%20+..+%20w_%7Bn%7Dx_%7Bn%7D%20%3D%20x%5E%7BT%7Dw)\n","<br/>\n","\n","kemudian hasil ***z*** itu diaplikasikan kepada fungsi ***step(z)*** untuk menghasilkan nilai output yang baharu,\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20h_%7Bw%7D%28x%29%20%3D%20step%28z%29%2C%20where%20%5C%20z%20%3D%20x%5E%7BT%7Dw)\n","<br/>\n","\n","Menyingkap kembali pada artikel yang [sebelum ini](https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/23/AI-ML-DL.html), ia masih menggunakan rumus yang sama iaitu ***y = ax + b*** tanpa pembolehubah ***b***. Merujuk pada ini, ada sedikit perbezaan iaitu nilai output adalah hasil dari 3 input fitur ***x*** dengan nilai pengaruh weight ***w*** (dahulunya ***a***) pada setiap fitur tersebut. Perbezaan yang paling utama adalah peranan ***Step Function*** sebagai \"***Activation Function***\". Idea *activation function* ini penting untuk membolehkan kita membina model yang ***non-linear*** (perceptron adalah algoritma linear untuk klasifikasi binari kerana nilai aktivasinya adalah 1 atau 0). Kenapa kita perlukan model *non-linear*? Cuba ingat balik fakta dari artikel yang lepas, garisan ***linear*** hanya mampu menghasilkan suatu model yang ***underfit*** atau ***high bias***. Ini bukan suatu model yang kita inginkan untuk menyelesaikan masalah yang kompleks."]},{"cell_type":"markdown","metadata":{"id":"50cMmuivJAOD"},"source":["Mungkin ada yang tertanya-tanya kenapa ada huruf *superscript* ***T*** di atas huruf ***x*** itu?\n","<br/>\n","<br/>\n",">![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20x%5E%7BT%7Dw)\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"809U3ai_UYKh"},"source":["Ini kerana ia melibatkan pengiraan menggunakan *Matrix Multiplication*. Janganlah rasa takut bila dengar terma matrix ini. Anda hanya perlu tahu asas berkenaan ilmu matematik *matrix* yang di pelajari di sekolah menengah. Lagipun semua pengiraan ini nanti akan dilakukan oleh library Tensorflow atau Pytorch. \n","\n","Ok. Huruf ***T*** adalah merujuk kepada proses matrix yang di panggil ***Transpose***. Ia diperlukan untuk membetulkan bentuk dimensi matrix ***x*** bagi membolehkan ia multiply dengan matrix ***w***. Contoh, jika bentuk dimensi matrix ***x*** dan ***w*** adalah ***(3,1)***, operasi matrix dimensi ***x(3,1)*** x ***w(3,1)*** itu perlu ditukarkan ke bentuk ***x(1,3)*** x ***w(3,1)*** terlebih dahulu dengan melakukan proses transpose pada matrix ***x***.\n","\n","Jika anda sudah lupa berkenaan operasi asas matrix, bolehlah rujuk kembali pada buku SPM anda. InsyaAllah, sayang anda pada cikgu matematik akan bertambah. "]},{"cell_type":"markdown","metadata":{"id":"mllUUhOqI1S-"},"source":["Ok. Bagaimana pula dengan Step Function? \n","\n","Cuba lihat notasi matematik di bawah. Ia merujuk kepada dua jenis fungsi Step bernama ***Heaviside*** dan ***Sign***. Dulu saya cukup fobia bila melihat notasi matematik dengan simbol macam cacing ni. Tapi sebenarnya jika anda cuba amati notasi tersebut ia sebenarnya adalah merupakan satu \"bahasa\" simbolik. Jika anda hafal maksud simbol-simbolnya maka mudahlah memahami maksudnya.\n","\n","Fungsi ***heaviside*** akan mengeluarkan nilai ***0*** jika nilai ***z*** adalah lebih kecil daripada ***0***. Sebaliknya, ia mengeluarkan nilai ***1*** jika nilai ***z*** adalah sama atau lebih besar daripada ***0***.\n","\n","\n","\n","<br/>\n","<br/>\n","\n","![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20heaviside%28z%29%3D%5Cbegin%7Bcases%7D%20%26%20%5Ctext%7B0%20if%20%7D%20z%3C%200%20%5C%5C%20%26%20%5Ctext%7B1%20if%20%7D%20z%5Cgeq%200%20%5Cend%7Bcases%7D)\n","\n","<br/>\n","\n","Manakala fungsi ***Sign*** pula akan mengeluarkan nilai ***-1*** jika nilai ***z*** adalah lebih kecil daripada ***0***. Ia juga mengeluarkan nilai ***0*** jika nilai ***z*** adalah ***0***, dan mengeluarkan nilai ***+1*** jika nilai ***z*** adalah lebih besar daripada ***0***.\n","\n","<br/>\n","\n","![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20sgn%28z%29%3D%5Cbegin%7Bcases%7D%20%26%20%5Ctext%7B%20-1%20if%20%7D%20z%3C%200%20%5C%5C%20%26%20%5Ctext%7B%20%5C%200%20if%20%7D%20z%3D%200%20%5C%5C%20%26%20%5Ctext%7B+1%20if%20%7D%20z%3E%200%20%5Cend%7Bcases%7D)\n","<br/>\n","\n","Untuk memberi gambaran jelas pada yang \"***visual learner***\", cuba lakarkan graf pada paksi ***z -> X*** dan ***sgn(z) -> Y***. Saya serahkan pada anda untuk melakarkannya."]},{"cell_type":"markdown","metadata":{"id":"XOb0P28RuHjl"},"source":["*Architecture* **TLU** tunggal ini boleh digunakan untuk klasifikasi binari yang mudah seperti **Hujan (1)** atau **Tidak (0)**. Ia menghitung kombinasi linear dari 3 input ***x*** (dengan nilai weight masing-masing), dan jika hasil ***z*** melebihi nilai ***treshold*** (***z >= 0***) seperti yang ditetapkan oleh contohnya fungsi ***heaviside***, ia akan menghasilkan prediksi kelas **positif (1)**. Jika sebaliknya, ia menghasilkan prediksi kelas **negatif (0)**. Melatih **TLU** dalam kes ini bermaksud mencari nilai yang optimum untuk *weight* ***w1***, ***w2***, dan ***w3*** itu.\n","\n","Perceptron hanya terdiri daripada satu lapisan TLU, dengan setiap TLU disambungkan ke semua input. Apabila semua neuron dalam lapisan disambungkan ke setiap neuron pada lapisan sebelumnya (iaitu, neuron inputnya), lapisan tersebut disebut ***fully connected layer***, atau ***dense layer***. Input fitur yang memasuki lapisan TLU itu dianggap sebagai lapisan input neuron ( tiada pemprosesan berlaku di neuron ini). Kebiasaannya, input ***bias***  di tambah (seperti nilai ***b*** pada ***y=ax+b*** untuk linear regression model bergerak ke atas dan bawah paksi y, tetapi untuk DL ada tujuan tambahan) menggunakan neuron khas yang disebut **neuron bias** (Ia ditetapkan dengan nilai 1). Gambarajah di bawah menunjukkan perceptron dengan dua input, satu input bias dan tiga output.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Xd-mb3yHk3c3"},"source":["<br/>\n","<br/>\n","<img src=\"fastcore_imgs/perceptron-2i-3o-1b.png\" width=\"80%\" height=\"80%\" alt=\"perceptron-2i-3o-1b\">\n","<br/>\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"ieVpIWPHlS_0"},"source":["Apa tujuan lain bias? kenapa nilainya disetkan dengan nilai 1?\n","\n","Anda tahu bahawa nilai output untuk 1 neuron TLU  dan 1 input ditentukan oleh input, weight dan activation function step *heaviside* seperti berikut,\n","\n",">***y = f (x₀ × w₀)*** \n"," \n","Sekiranya input adalah ***x₀ = 0*** maka ***y = f (0) = 1***.\n","\n","Ini menyebabkan neutron tersebut sentiasa menghasilkan output yang sama (1) walaupun nilai weight ***w₀*** berubah-ubah. Ia menyebabkan neuron itu berhenti belajar (neuron tidak aktif). Nilai bias=1 (***y = f (x₀ × w₀ + 1)*** ), memastikan supaya neuron itu masih aktif walaupun nilai inputnya 0 (bias mengubah takat treshold atau trigger value activation function untuk neuron tersebut). \n","\n","Semestinya nilai bias=1 sahaja ker?  Tidak!\n","\n","Bias juga adalah *learnable* parameter seperti *weight* parameter. Nilai 1 itu adalah tetapan permulaannya sahaja. Merujuk kepada gambarajah perceptron di atas, rumus untuk menghitung output lapisan ***fully connected *** adalah seperti berikut.\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/png.download?%5Clarge%20h_%7Bw%2Cb%7D%5Cleft%20%28%20X%20%5Cright%20%29%3D%5Cphi%20%5Cleft%20%28%20XW+b%20%5Cright%20%29)\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"Fnmhdh71Hold"},"source":["Penerangan rumus:\n","\n","* ***X*** mewakili matrix fitur input. Ia mempunyai satu *row* pada setiap rekod (juga di panggil *sample* atau *instance*) dan satu column pada setiap fitur (Awak bayangkan row dan column pada MS Excel). Merujuk pada gambarajah perceptron di atas, jika kita ada 1000 instance, maka saiz matrik ***X*** adalah (1000, 2).\n","* Matrix weight ***W*** mengandungi semua sambungan weight antara neuron input dan neuron output kecuali neuron bias. Ia mempunyai satu row bagi setiap neuron input dan satu column bagi setiap neuron TLU lapisan output. Merujuk pada gambarajah di atas (kita ada 2 neuron input dan 3 neuron output), maka saiz matrix ***W*** adalah (2,3).\n","* Vektor bias ***b*** mengandungi semua hubungan weight antara neuron bias dan neuron ouput. Iaitu ia mempunyai satu neuron bias bagi setiap neuron output TLU.\n","* Fungsi φ ialah fungsi *activation*. TLU menggunakan fungsi step sebagai fungsi *activation*nya (kita akan membincangkan fungsi activation lain pada artikel yang seterusnya)."]},{"cell_type":"markdown","metadata":{"id":"8Ltu5cqwWF9E"},"source":["Bagaimanakah proses training dilakukan oleh perceptron?\n","\n","Konsepnya adalah sama dengan yang telah saya jelaskan pada artikel yang [sebelum ini](https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/23/AI-ML-DL.html). Seperti contoh, setiap data instance atau sample itu dilabelkan dengan nilai target seperti contoh, Hujan(1) & Tidak(0),\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/gif.download?%5Clarge%20%5C%7Bx_%7B1%7D%2Cx_%7B2%7D%2Ctarget%5C%7D%5Crightarrow%20%5C%7B32%2C25%2C1%5C%7D%2C%5C%7B31%2C24%2C1%5C%7D%5Ccdots%20%5C%7B50%2C45%2C0%5C%7D%2C%5C%7B51%2C44%2C0%5C%7D)\n","<br/>\n","Dalam proses training, hanya satu instance atau sample di sumbat masuk pada satu masa. Namun hanya input fitur ***x*** (tanpa nilai target) sahaja yang akan disambungkan pada neuron TLU. Manakala nilai target daripada instance itu akan digunakan untuk mengukur error (nilai ramalan - target label). Output TLU neuron menghasilkan (melalui fungsi step) nilai ramalan sama ada ***1*** atau ***0*** berdasarkan pada variasi nilai weight ***w***. Pada setiap kali (melalui proses iteration) neuron output itu menghasilkan ramalan yang salah, ia akan mempertingkatkan nilai weight ***w*** antara neuron input dan TLU tersebut sehingga ramalannya menjadi betul. Iaitu pada setiap iteration, nilai weight ***w*** akan diupdatekan (lihat rumus di bawah) mengikut error sama ada ke arah 1 atau 0.\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/gif.download?%5Clarge%20w_%7Bij%7D%3Dw_%7Bij%7D+%5CDelta%20w_%7Bij%7D)\n","<br/>\n","Tetapi macam mana nak dikaitkan nilai \n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/gif.download?%5Clarge%20%5CDelta%20w_%7Bij%7D)\n","<br/>\n","dengan nilai error (ramalan - target label)? Secara logiknya adalah untuk mengurangkan weight ***w*** jika neuron diaktifkan (1) walaupun ia sepatutnya tidak (0) diaktifkan, (ramalan: y = 1 dan target: t = 0). Sebaliknya dengan meningkatkan weight ***w*** jika neuron tidak (0) diaktifkan walaupun seharusnya diaktifkan (1), (ramalan: y = 0 dan target: t = 1).\n","\n","Oleh itu kita boleh gantikan dengan\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/gif.download?%5Clarge%20%5CDelta%20w_%7Bij%7D%3D-%28y-t%29)\n","<br/>\n","Iaitu, jika sepatutnya aktif (1): \n","\n","<br/>\n",">***Δw(i,j)=-(0–1)=1 (tingkatkan weight w)*** \n","<br/>\n","\n","dan jika tidak sepatutnya aktif (0): \n","\n","<br/>\n",">***Δw(i,j)=-(1–0)=-1 (kurangkan weight w)***\n","<br/>\n","\n","Kita masih lagi satu masalah, bagaimana kalau nilai input adalah negative?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qaWrmIeo8Xd5","executionInfo":{"status":"ok","timestamp":1609741193571,"user_tz":-480,"elapsed":1167,"user":{"displayName":"Azmi Megat","photoUrl":"","userId":"09952198438441952493"}},"outputId":"7f6b1ff5-754a-4651-8053-e4e9ef4145a7"},"source":["import numpy as np\n","from sklearn.datasets import load_iris \n","from sklearn.linear_model import Perceptron\n","\n","iris = load_iris() # load iris data\n","print(iris.target)\n","X = iris.data[:, (2, 3)] # petal length, petal width y = (iris.target == 0).astype(np.int) # Iris setosa?\n","y = (iris.target == 2).astype(np.int) # Iris setosa?\n","print(X,y)\n","per_clf = Perceptron()\n","per_clf.fit(X, y)\n","y_pred = per_clf.predict([[5.1, 1.8]])\n","print(y_pred)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2]\n","[[1.4 0.2]\n"," [1.4 0.2]\n"," [1.3 0.2]\n"," [1.5 0.2]\n"," [1.4 0.2]\n"," [1.7 0.4]\n"," [1.4 0.3]\n"," [1.5 0.2]\n"," [1.4 0.2]\n"," [1.5 0.1]\n"," [1.5 0.2]\n"," [1.6 0.2]\n"," [1.4 0.1]\n"," [1.1 0.1]\n"," [1.2 0.2]\n"," [1.5 0.4]\n"," [1.3 0.4]\n"," [1.4 0.3]\n"," [1.7 0.3]\n"," [1.5 0.3]\n"," [1.7 0.2]\n"," [1.5 0.4]\n"," [1.  0.2]\n"," [1.7 0.5]\n"," [1.9 0.2]\n"," [1.6 0.2]\n"," [1.6 0.4]\n"," [1.5 0.2]\n"," [1.4 0.2]\n"," [1.6 0.2]\n"," [1.6 0.2]\n"," [1.5 0.4]\n"," [1.5 0.1]\n"," [1.4 0.2]\n"," [1.5 0.2]\n"," [1.2 0.2]\n"," [1.3 0.2]\n"," [1.4 0.1]\n"," [1.3 0.2]\n"," [1.5 0.2]\n"," [1.3 0.3]\n"," [1.3 0.3]\n"," [1.3 0.2]\n"," [1.6 0.6]\n"," [1.9 0.4]\n"," [1.4 0.3]\n"," [1.6 0.2]\n"," [1.4 0.2]\n"," [1.5 0.2]\n"," [1.4 0.2]\n"," [4.7 1.4]\n"," [4.5 1.5]\n"," [4.9 1.5]\n"," [4.  1.3]\n"," [4.6 1.5]\n"," [4.5 1.3]\n"," [4.7 1.6]\n"," [3.3 1. ]\n"," [4.6 1.3]\n"," [3.9 1.4]\n"," [3.5 1. ]\n"," [4.2 1.5]\n"," [4.  1. ]\n"," [4.7 1.4]\n"," [3.6 1.3]\n"," [4.4 1.4]\n"," [4.5 1.5]\n"," [4.1 1. ]\n"," [4.5 1.5]\n"," [3.9 1.1]\n"," [4.8 1.8]\n"," [4.  1.3]\n"," [4.9 1.5]\n"," [4.7 1.2]\n"," [4.3 1.3]\n"," [4.4 1.4]\n"," [4.8 1.4]\n"," [5.  1.7]\n"," [4.5 1.5]\n"," [3.5 1. ]\n"," [3.8 1.1]\n"," [3.7 1. ]\n"," [3.9 1.2]\n"," [5.1 1.6]\n"," [4.5 1.5]\n"," [4.5 1.6]\n"," [4.7 1.5]\n"," [4.4 1.3]\n"," [4.1 1.3]\n"," [4.  1.3]\n"," [4.4 1.2]\n"," [4.6 1.4]\n"," [4.  1.2]\n"," [3.3 1. ]\n"," [4.2 1.3]\n"," [4.2 1.2]\n"," [4.2 1.3]\n"," [4.3 1.3]\n"," [3.  1.1]\n"," [4.1 1.3]\n"," [6.  2.5]\n"," [5.1 1.9]\n"," [5.9 2.1]\n"," [5.6 1.8]\n"," [5.8 2.2]\n"," [6.6 2.1]\n"," [4.5 1.7]\n"," [6.3 1.8]\n"," [5.8 1.8]\n"," [6.1 2.5]\n"," [5.1 2. ]\n"," [5.3 1.9]\n"," [5.5 2.1]\n"," [5.  2. ]\n"," [5.1 2.4]\n"," [5.3 2.3]\n"," [5.5 1.8]\n"," [6.7 2.2]\n"," [6.9 2.3]\n"," [5.  1.5]\n"," [5.7 2.3]\n"," [4.9 2. ]\n"," [6.7 2. ]\n"," [4.9 1.8]\n"," [5.7 2.1]\n"," [6.  1.8]\n"," [4.8 1.8]\n"," [4.9 1.8]\n"," [5.6 2.1]\n"," [5.8 1.6]\n"," [6.1 1.9]\n"," [6.4 2. ]\n"," [5.6 2.2]\n"," [5.1 1.5]\n"," [5.6 1.4]\n"," [6.1 2.3]\n"," [5.6 2.4]\n"," [5.5 1.8]\n"," [4.8 1.8]\n"," [5.4 2.1]\n"," [5.6 2.4]\n"," [5.1 2.3]\n"," [5.1 1.9]\n"," [5.9 2.3]\n"," [5.7 2.5]\n"," [5.2 2.3]\n"," [5.  1.9]\n"," [5.2 2. ]\n"," [5.4 2.3]\n"," [5.1 1.8]] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1]\n","[1]\n"],"name":"stdout"}]}]}