{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2020-12-29-basic-dl.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMfC3fezEHs/D4EIGQqYFzV"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sz_uQONCoQW7"},"source":["# Pengenalan Kepada Deep Learning\n","> Perbincangan berkenaan konsep asas deep learning\n","\n","- toc: true \n","- badges: false\n","- comments: true\n","- categories: [deep learning]\n","- image: images/\n","- author: Megat Norulazmi"]},{"cell_type":"markdown","metadata":{"id":"ReYmjuFcpi_v"},"source":["Assalamualaikum. Selamat berjumpa kembali.\n","\n","Seperti yang saya maklumkan pada [artikel yang lepas](https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/23/AI-ML-DL.html), saya telah berjanji untuk cuba memberikan sedikit penerangan asas berkenaan teori DL ini. Kalau nak diikutkan, jika kita menggunakan library tensorflow atau pytorch nanti, kesemua yang kita bincang ini akan diabstrakkan implementasinya. \n","\n","Dengan kata lain, \"Tak payah tau pun, pakai jer\". \n","\n","Tapi saya rasa anda perlu tahu juga konsep asasnya terlebih dahulu, sebab ini akan membantu anda untuk majukan ke tahap penggunaan DL yang lebih kompleks. Lagipun, tak kanlah nak pakai jer library tensorflow atau pytorch membuta tuli kan?"]},{"cell_type":"markdown","metadata":{"id":"BfobCvuKM1fT"},"source":["# Perceptron\n","***Perceptron*** adalah salah satu *architecture* ***ANN***   yang paling asas, dicipta pada tahun 1957 oleh Frank Rosenblatt. Ia berdasarkan pada konsep *neuron* (lihat gambar di bawah) yang disebut ***Treshold Logic Unit*** (TLU), ataupun ***Logic Treshold Unit*** (LTU). Nilai input dan outputnya adalah nombor (bukan nilai on/off binari), dan setiap sambungan di antara input nod dan output nod itu diberikan nilai pemberat atau ***weight***. Secara analoginya, lagi besar *weight* maka lagi tebal lah wayar sambungannya (Lagi besar pengaruhnya). \n","TLU menghitung jumlah *weight* beserta dengan inputnya seperti yang ditunjukkan oleh rumus di bawah,\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20z%3Dw_%7B1%7Dx_%7B1%7D+w_%7B2%7Dx_%7B2%7D+..+w_%7Bn%7Dx_%7Bn%7D%3Dx%5E%7BT%7Dw)\n","<br/>\n","\n","kemudian hasil ***z*** itu diaplikasikan kepada fungsi ***step(z)*** untuk menhasilkan nilai output yang baharu,\n","<br/>\n","<br/>\n","![equation](https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20hw%28x%29%3Dstep%28z%29%2C%20where%5C%20z%3Dx%5E%7BT%7Dw)\n","<br/>\n","\n","Menyingkap kembali pada artikel yang [sebelum ini](https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/23/AI-ML-DL.html), ia masih menggunakan rumus yang sama iaitu ***y = Ax + b*** tanpa pembolehubah ***b***. Bezanya kali ini adalah nilai output adalah hasil dari 3 input fitur ***x*** dengan nilai pengaruh weight ***w*** (dahulunya ***a***) pada setiap fitur tersebut. Apa yang membezakannya adalah peranan ***Step Function*** ataupun \"***Activation Function***\" (terma yang digunakan dalam DL) tersebut. Idea *activation function* ini penting untuk membolehkan kita membina model yang ***non-linear***. Kenapa kita perlukan model *non-linear*? Cuba ingat balik fakta dari artikel yang lepas, garisan ***linear*** hanya mampu menghasilkan suatu model yang ***underfit*** atau ***high bias***. Ini bukan suatu model yang kita inginkan untuk menyelesaikan masalah yang kompleks.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Da_Z9aQwUfCH"},"source":["<br/>\n","<br/>\n","<img src=\"fastcore_imgs/perceptron.png\" width=\"80%\" height=\"80%\" alt=\"perceptron\">\n","<br/>\n","<br/>"]}]}