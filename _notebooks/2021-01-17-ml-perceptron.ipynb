{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021-01-17-ml-perceptron.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNeYwlxiM2FRJZiMx0rUv9p"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KSsw1MZdreyW"},"source":["# Pengenalan Kepada Multilayer Perceptron & Backpropagation\n","\n","> Perbincangan berkenaan kelemahan perceptron dan bagaimana multilayer perceptron memperbaiki kelemahan tersebut dengan bantuan backpropagation\n","\n","- toc: false \n","- badges: false\n","- comments: true\n","- categories: [deep learning]\n","- image: images/\n","- author: Megat Norulazmi"]},{"cell_type":"markdown","metadata":{"id":"BJQeSjhjswRZ"},"source":["Assalamualaikum dan selamat berjumpa kembali.\n","\n","Pada [artikel yang lepas](https://megatazm.github.io/MyDLBlog/deep%20learning/2021/01/11/perceptron-ml-sklearn.html), kita telah menggunakan perceptron sebagai contoh algoritma di dalam proses machine learning. Seperti yang anda ketahui, algoritma perceptron ini telah di cipta hampir 60 tahun yang lampau. Ia tidak lah cukup berkuasa untuk menyelesai keperluan masa kini yang kompleks dan mencabar. Malahan, kelemahan perceptron yang di temui oleh Marvin Minsky and Seymour Papert pada tahun 1969 telah menyebabkan algoritma berdasarkan pada konsep neural network ini terkubur hampir dua dekad lamanya. \n","\n","Kenapa? <(^,^)>\n","\n","Jika anda membaca artikel saya sebelum ini, ada dinyatakan bahawa perceptron hanya mampu melakukan klasifikasi binari yang datanya adalah ***lineary separable***. \n","\n","Masih ingat? (;¬_¬)\n","\n","Kekangan yang ada pada perceptron ini digambarkan seperti di bawah melalui apa yang dikenali sebagai masalah XOR. Gambarajah di bawah juga menunjukkan perbandingan antara 2 fungsi logik OR dan XOR."]},{"cell_type":"markdown","metadata":{"id":"6fJZoRsb2EJr"},"source":["<br/>\n","<br/>\n","<img src=\"fastcore_imgs/xor-problem.png\" width=\"100%\" height=\"100%\" alt=\"xor problem\">\n","<br/>\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"rqLjvVCI2KlN"},"source":["Di bawah adalah **Truth Table** menunjukkan hubungan input & output yang dihasilkan oleh fungsi logik OR dan XOR (Merujuk pada gambarajah di atas, simbol dot merah = 0 manakala simbol plus biru = 1). "]},{"cell_type":"markdown","metadata":{"id":"hKa9kTlA7Yf-"},"source":["<br/>\n","<br/>\n","<img src=\"fastcore_imgs/xor-or-truthtable.png\" width=\"100%\" height=\"100%\" alt=\"xor-or-truthtable\">\n","<br/>\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"8BO9aj8K8jui"},"source":["Sekiranya kita train perceptron untuk memperolehi model yang berkebolehan seperti fungsi **OR** dan **XOR** ini, kita dapat melihat pada gambarajah di plot kiri (fungsi OR), bahawa perceptron boleh melakukannya melalui satu garisan linear. Namun, pada plot di kanan (fungsi XOR), perceptron tidak mampu melakukanya kerana fungsi XOR adalah dikatakan \"***linearly non-separable***\". Satu-satunya cara untuk memisahkan kedua-dua kelas (biru dan merah) ini adalah dengan menggunakan garisan sempadan bukan linear. Ini jelas menunjukkan bahawa perceptron boleh mempelajari fungsi OR tetapi tidak dapat mempelajari fungsi XOR. \n","\n","Sungguh mengecewakan! ╭∩╮(︶︿︶)╭∩╮"]},{"cell_type":"markdown","metadata":{"id":"BKEkB_OYDr_l"},"source":["Secara teorinya kelemahan perceptron itu dapat diselesaikan dengan menambah satu lagi lapisan neuron diatasnya. Ini menghasilkan apa yang disebut sebagai **Multilayer Perceptron** (**MLP**). Gambarajah di bawah menunjukkan bagaimana MLP dapat menyelesaikan masalah XOR. Melalui proses training, gambarajah di bawah menunjukkan nilai weight pada semua sambungan (anak panah) NN yang berwarna hitam adalah 1, melainkan yang berwarna merah iaitu dengan nilai seperti di gambarajah. Jika kita menghitung output MLP itu dengan input (0, 0) atau (1, 1), hasil output=0. Manakala jika dengan input (0, 1) atau (1, 0) ia menghasilkan output=1."]},{"cell_type":"markdown","metadata":{"id":"BDvRqme0NFtd"},"source":["<br/>\n","<br/>\n","<img src=\"fastcore_imgs/mlp-solved-xor.png\" width=\"100%\" height=\"100%\" alt=\"mlp-solved-xor\">\n","<br/>\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"JFI-3-pG9a3A"},"source":["Boleh nampak tak? Ok lah saya tunjukkan pengiraannya dengan terperinchi. Sebelum itu, sila rujuk pada penerangan berkenaan perceptron di [artikel saya yang sebelum ini](https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/29/basic-dl.html). Di bawah adalah rumus yang digunakan untuk mengira output yang keluar di lapisan neuron kedua (hidden layer) dan ketiga (output layer)."]},{"cell_type":"markdown","metadata":{"id":"Ln_kFrtmn4Gm"},"source":["<br/>\n","\n","![equation](https://latex.codecogs.com/png.download?%5Clarge%20%5Cbegin%7Bmatrix%7D%20%5C%5C%20f_%7Bs_%7Bi%7D%7D%5Cleft%20%28%20z%20%5Cright%20%29%3DWX%20+%20B%20%3D%20%5Cbegin%7Bbmatrix%7D%20w_%7B1%2C1%7D%20%26%20w_%7B1%2C2%7D%20%5C%5C%20w_%7B2%2C1%7D%20%26%20w_%7B2%2C2%7D%20%5C%5C%20%5Cend%7Bbmatrix%7D%5Cbegin%7Bbmatrix%7D%20x_%7B1%7D%20%5C%5C%20x_%7B2%7D%20%5C%5C%20%5Cend%7Bbmatrix%7D%20+%20%5Cbegin%7Bbmatrix%7D%20b_%7B1%7D%20%5C%5C%20b_%7B2%7D%20%5C%5C%20%5Cend%7Bbmatrix%7D%20%5C%5C%20%5C%5C%20%3D%5Cbegin%7Bbmatrix%7D%20w_%7B1%2C1%7D%5Ctimes%20x_%7B1%7D%20+%20w_%7B1%2C2%7D%5Ctimes%20x_%7B2%7D%20%5C%5C%20w_%7B2%2C1%7D%5Ctimes%20x_%7B1%7D%20+%20w_%7B2%2C2%7D%5Ctimes%20x_%7B2%7D%20%5C%5C%20%5Cend%7Bbmatrix%7D+%5Cbegin%7Bbmatrix%7D%20b_%7B1%7D%20%5C%5C%20b_%7B2%7D%20%5C%5C%20%5Cend%7Bbmatrix%7D%20%5Cend%7Bmatrix%7D)\n","<br/>"]},{"cell_type":"markdown","metadata":{"id":"29RXnzr9qS8R"},"source":["Mari hitung nilai output neuron 1 dan neuron 2 di hidden layer.\n","\n","* input ***x1=0*** dan ***x2=0***; \n","* weight w11=,w12=,w21=, dan w22=\n"]}]}