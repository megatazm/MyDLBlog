<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Pengenalan Kepada Perceptron | fastpages</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Pengenalan Kepada Perceptron" />
<meta name="author" content="Megat Norulazmi" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Perbincangan berkenaan konsep asas deep learning" />
<meta property="og:description" content="Perbincangan berkenaan konsep asas deep learning" />
<link rel="canonical" href="https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/29/basic-dl.html" />
<meta property="og:url" content="https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/29/basic-dl.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:image" content="https://megatazm.github.io/MyDLBlog/images/" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-29T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Perbincangan berkenaan konsep asas deep learning","url":"https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/29/basic-dl.html","@type":"BlogPosting","headline":"Pengenalan Kepada Perceptron","dateModified":"2020-12-29T00:00:00-06:00","datePublished":"2020-12-29T00:00:00-06:00","author":{"@type":"Person","name":"Megat Norulazmi"},"image":"https://megatazm.github.io/MyDLBlog/images/","mainEntityOfPage":{"@type":"WebPage","@id":"https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/29/basic-dl.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/MyDLBlog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://megatazm.github.io/MyDLBlog/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/MyDLBlog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/MyDLBlog/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/MyDLBlog/about/">About Me</a><a class="page-link" href="/MyDLBlog/search/">Search</a><a class="page-link" href="/MyDLBlog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Pengenalan Kepada Perceptron</h1><p class="page-description">Perbincangan berkenaan konsep asas deep learning</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-12-29T00:00:00-06:00" itemprop="datePublished">
        Dec 29, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Megat Norulazmi</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      11 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/MyDLBlog/categories/#deep learning">deep learning</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#Perceptron">Perceptron </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-12-29-basic-dl.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Assalamualaikum. Selamat berjumpa kembali.</p>
<p>Seperti yang saya maklumkan pada <a href="https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/23/AI-ML-DL.html">artikel yang lepas</a>, saya telah berjanji untuk cuba memberikan sedikit penerangan asas berkenaan teori DL ini. Kalau nak diikutkan, jika kita menggunakan library Tensorflow atau Pytorch nanti, kesemua yang kita bincang ini akan diabstrakkan implementasinya.</p>
<p>Dengan kata lain, "Tak payah tau pun, pakai jer".</p>
<p>Tapi saya rasa anda perlu tahu juga konsep asasnya terlebih dahulu, sebab ini akan membantu anda untuk majukan ke tahap penggunaan DL yang lebih kompleks. Lagipun, tak kanlah nak pakai jer library Tensorflow atau Pytorch membuta tuli kan?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Perceptron">
<a class="anchor" href="#Perceptron" aria-hidden="true"><span class="octicon octicon-link"></span></a>Perceptron<a class="anchor-link" href="#Perceptron"> </a>
</h1>
<p><strong><em>Perceptron</em></strong> adalah salah satu <em>architecture</em> <strong><em>ANN</em></strong>   yang paling asas, dicipta pada tahun 1957 oleh Frank Rosenblatt. Ia berdasarkan pada konsep <em>neuron</em> (lihat gambar di bawah) yang disebut <strong><em>Treshold Logic Unit</em></strong> (TLU), ataupun <strong><em>Logic Treshold Unit</em></strong> (LTU). Nilai input dan outputnya adalah nombor (bukan nilai on/off binari), dan setiap sambungan di antara input nod dan output nod itu diberikan nilai pemberat atau <strong><em>weight</em></strong>. Secara analoginya, lagi besar <em>weight</em> maka lagi tebal lah wayar sambungannya (Lagi besar pengaruhnya).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br>
<br>
<figure>
  
    <img class="docimage" src="/MyDLBlog/images/copied_from_nb/fastcore_imgs/perceptron.png" alt="perceptron" style="max-width: 80%px">
    
    
</figure>

<br>
<br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>TLU menghitung jumlah <em>weight</em> berserta dengan inputnya seperti yang ditunjukkan oleh rumus di bawah,
<br>
<br>
<img src="https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20z%3D%20w_%7B1%7Dx_%7B1%7D%20+%20w_%7B2%7Dx_%7B2%7D%20+..+%20w_%7Bn%7Dx_%7Bn%7D%20%3D%20x%5E%7BT%7Dw" alt="equation">
<br></p>
<p>kemudian hasil <strong><em>z</em></strong> itu diaplikasikan kepada fungsi <strong><em>step(z)</em></strong> untuk menghasilkan nilai output yang baharu,
<br>
<br>
<img src="https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20h_%7Bw%7D%28x%29%20%3D%20step%28z%29%2C%20where%20%5C%20z%20%3D%20x%5E%7BT%7Dw" alt="equation">
<br></p>
<p>Menyingkap kembali pada artikel yang <a href="https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/23/AI-ML-DL.html">sebelum ini</a>, ia masih menggunakan rumus yang sama iaitu <strong><em>y = ax + b</em></strong> tanpa pembolehubah <strong><em>b</em></strong>. Merujuk pada ini, ada sedikit perbezaan iaitu nilai output adalah hasil dari 3 input fitur <strong><em>x</em></strong> dengan nilai pengaruh weight <strong><em>w</em></strong> (dahulunya <strong><em>a</em></strong>) pada setiap fitur tersebut. Perbezaan yang paling utama adalah peranan <strong><em>Step Function</em></strong> sebagai "<strong><em>Activation Function</em></strong>". Idea <em>activation function</em> ini penting untuk membolehkan kita membina model yang <strong><em>non-linear</em></strong> (perceptron adalah algoritma linear untuk klasifikasi binari kerana nilai aktivasinya adalah 1 atau 0). Kenapa kita perlukan model <em>non-linear</em>? Cuba ingat balik fakta dari artikel yang lepas, garisan <strong><em>linear</em></strong> hanya mampu menghasilkan suatu model yang <strong><em>underfit</em></strong> atau <strong><em>high bias</em></strong>. Ini bukan suatu model yang kita inginkan untuk menyelesaikan masalah yang kompleks.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Mungkin ada yang tertanya-tanya kenapa ada huruf <em>superscript</em> <strong><em>T</em></strong> di atas huruf <strong><em>x</em></strong> itu?
<br>
<br></p>
<blockquote>
<p><img src="https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20x%5E%7BT%7Dw" alt="equation"><br></p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Ini kerana ia melibatkan pengiraan menggunakan <em>Matrix Multiplication</em>. Janganlah rasa takut bila dengar terma matrix ini. Anda hanya perlu tahu asas berkenaan ilmu matematik <em>matrix</em> yang di pelajari di sekolah menengah. Lagipun semua pengiraan ini nanti akan dilakukan oleh library Tensorflow atau Pytorch.</p>
<p>Ok. Huruf <strong><em>T</em></strong> adalah merujuk kepada proses matrix yang di panggil <strong><em>Transpose</em></strong>. Ia diperlukan untuk membetulkan bentuk dimensi matrix <strong><em>x</em></strong> bagi membolehkan ia multiply dengan matrix <strong><em>w</em></strong>. Contoh, jika bentuk dimensi matrix <strong><em>x</em></strong> dan <strong><em>w</em></strong> adalah <strong><em>(3,1)</em></strong>, operasi matrix dimensi <strong><em>x(3,1)</em></strong> x <strong><em>w(3,1)</em></strong> itu perlu ditukarkan ke bentuk <strong><em>x(1,3)</em></strong> x <strong><em>w(3,1)</em></strong> terlebih dahulu dengan melakukan proses transpose pada matrix <strong><em>x</em></strong>.</p>
<p>Jika anda sudah lupa berkenaan operasi asas matrix, bolehlah rujuk kembali pada buku SPM anda. InsyaAllah, sayang anda pada cikgu matematik akan bertambah.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Ok. Bagaimana pula dengan Step Function?</p>
<p>Cuba lihat notasi matematik di bawah. Ia merujuk kepada dua jenis fungsi Step bernama <strong><em>Heaviside</em></strong> dan <strong><em>Sign</em></strong>. Dulu saya cukup fobia bila melihat notasi matematik dengan simbol macam cacing ni. Tapi sebenarnya jika anda cuba amati notasi tersebut ia sebenarnya adalah merupakan satu "bahasa" simbolik. Jika anda hafal maksud simbol-simbolnya maka mudahlah memahami maksudnya.</p>
<p>Fungsi <strong><em>heaviside</em></strong> akan mengeluarkan nilai <strong><em>0</em></strong> jika nilai <strong><em>z</em></strong> adalah lebih kecil daripada <strong><em>0</em></strong>. Sebaliknya, ia mengeluarkan nilai <strong><em>1</em></strong> jika nilai <strong><em>z</em></strong> adalah sama atau lebih besar daripada <strong><em>0</em></strong>.</p>
<p><br>
<br></p>
<p><img src="https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20heaviside%28z%29%3D%5Cbegin%7Bcases%7D%20%26%20%5Ctext%7B0%20if%20%7D%20z%3C%200%20%5C%5C%20%26%20%5Ctext%7B1%20if%20%7D%20z%5Cgeq%200%20%5Cend%7Bcases%7D" alt="equation"></p>
<p><br></p>
<p>Manakala fungsi <strong><em>Sign</em></strong> pula akan mengeluarkan nilai <strong><em>-1</em></strong> jika nilai <strong><em>z</em></strong> adalah lebih kecil daripada <strong><em>0</em></strong>. Ia juga mengeluarkan nilai <strong><em>0</em></strong> jika nilai <strong><em>z</em></strong> adalah <strong><em>0</em></strong>, dan mengeluarkan nilai <strong><em>+1</em></strong> jika nilai <strong><em>z</em></strong> adalah lebih besar daripada <strong><em>0</em></strong>.</p>
<p><br></p>
<p><img src="https://latex.codecogs.com/png.download?%5Cdpi%7B100%7D%20%5Cfn_phv%20%5Clarge%20sgn%28z%29%3D%5Cbegin%7Bcases%7D%20%26%20%5Ctext%7B%20-1%20if%20%7D%20z%3C%200%20%5C%5C%20%26%20%5Ctext%7B%20%5C%200%20if%20%7D%20z%3D%200%20%5C%5C%20%26%20%5Ctext%7B+1%20if%20%7D%20z%3E%200%20%5Cend%7Bcases%7D" alt="equation">
<br></p>
<p>Untuk memberi gambaran jelas pada yang "<strong><em>visual learner</em></strong>", cuba lakarkan graf pada paksi <strong><em>z -&gt; X</em></strong> dan <strong><em>sgn(z) -&gt; Y</em></strong>. Saya serahkan pada anda untuk melakarkannya.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>Architecture</em> <strong>TLU</strong> tunggal ini boleh digunakan untuk klasifikasi binari yang mudah seperti <strong>Hujan (1)</strong> atau <strong>Tidak (0)</strong>. Ia menghitung kombinasi linear dari 3 input <strong><em>x</em></strong> (dengan nilai weight masing-masing), dan jika hasil <strong><em>z</em></strong> melebihi nilai <strong><em>treshold</em></strong> (<strong><em>z &gt;= 0</em></strong>) seperti yang ditetapkan oleh contohnya fungsi <strong><em>heaviside</em></strong>, ia akan menghasilkan prediksi kelas <strong>positif (1)</strong>. Jika sebaliknya, ia menghasilkan prediksi kelas <strong>negatif (0)</strong>. Melatih <strong>TLU</strong> dalam kes ini bermaksud mencari nilai yang optimum untuk <em>weight</em> <strong><em>w1</em></strong>, <strong><em>w2</em></strong>, dan <strong><em>w3</em></strong> itu.</p>
<p>Perceptron hanya terdiri daripada satu lapisan TLU, dengan setiap TLU disambungkan ke semua input. Apabila semua neuron dalam lapisan disambungkan ke setiap neuron pada lapisan sebelumnya (iaitu, neuron inputnya), lapisan tersebut disebut <strong><em>fully connected layer</em></strong>, atau <strong><em>dense layer</em></strong>. Input fitur yang memasuki lapisan TLU itu dianggap sebagai lapisan input neuron ( tiada pemprosesan berlaku di neuron ini). Kebiasaannya, input <strong><em>bias</em></strong>  di tambah (seperti nilai <strong><em>b</em></strong> pada <strong><em>y=ax+b</em></strong> untuk linear regression model bergerak ke atas dan bawah paksi y, tetapi untuk DL ada tujuan tambahan) menggunakan neuron khas yang disebut <strong>neuron bias</strong> (Ia ditetapkan dengan nilai 1). Gambarajah di bawah menunjukkan perceptron dengan dua input, satu input bias dan tiga output.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><br>
<br>
<figure>
  
    <img class="docimage" src="/MyDLBlog/images/copied_from_nb/fastcore_imgs/perceptron-2i-3o-1b.png" alt="perceptron-2i-3o-1b" style="max-width: 80%px">
    
    
</figure>

<br>
<br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Apa tujuan lain bias? kenapa nilainya disetkan dengan nilai 1?</p>
<p>Anda tahu bahawa nilai output untuk 1 neuron TLU  dan 1 input ditentukan oleh input, weight dan activation function step <em>heaviside</em> seperti berikut,</p>
<blockquote>
<p><strong><em>y = f (x₀ × w₀)</em></strong></p>
</blockquote>
<p>Sekiranya input adalah <strong><em>x₀ = 0</em></strong> maka <strong><em>y = f (0) = 1</em></strong>.</p>
<p>Ini menyebabkan neutron tersebut sentiasa menghasilkan output yang sama (1) walaupun nilai weight <strong><em>w₀</em></strong> berubah-ubah. Ia menyebabkan neuron itu berhenti belajar (neuron tidak aktif). Nilai bias=1 (<strong><em>y = f (x₀ × w₀ + 1)</em></strong> ), memastikan supaya neuron itu masih aktif walaupun nilai inputnya 0 (bias mengubah takat treshold atau trigger value activation function untuk neuron tersebut).</p>
<p>Semestinya nilai bias=1 sahaja ker?  Tidak!</p>
<p>Bias juga adalah <em>learnable</em> parameter seperti <em>weight</em> parameter. Nilai 1 itu adalah tetapan permulaannya sahaja. Merujuk kepada gambarajah perceptron di atas, rumus untuk menghitung output lapisan <strong><em>fully connected </em></strong> adalah seperti berikut.
<br>
<br>
<img src="https://latex.codecogs.com/png.download?%5Clarge%20h_%7Bw%2Cb%7D%5Cleft%20%28%20X%20%5Cright%20%29%3D%5Cphi%20%5Cleft%20%28%20XW+b%20%5Cright%20%29" alt="equation"><br></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Penerangan rumus:</p>
<ul>
<li>
<strong><em>X</em></strong> mewakili matrix fitur input. Ia mempunyai satu <em>row</em> pada setiap rekod (juga di panggil <em>sample</em> atau <em>instance</em>) dan satu column pada setiap fitur (Awak bayangkan row dan column pada MS Excel). Merujuk pada gambarajah perceptron di atas, jika kita ada 1000 instance, maka saiz matrik <strong><em>X</em></strong> adalah (1000, 2).</li>
<li>Matrix weight <strong><em>W</em></strong> mengandungi semua sambungan weight antara neuron input dan neuron output kecuali neuron bias. Ia mempunyai satu row bagi setiap neuron input dan satu column bagi setiap neuron TLU lapisan output. Merujuk pada gambarajah di atas (kita ada 2 neuron input dan 3 neuron output), maka saiz matrix <strong><em>W</em></strong> adalah (2,3).</li>
<li>Vektor bias <strong><em>b</em></strong> mengandungi semua hubungan weight antara neuron bias dan neuron ouput. Iaitu ia mempunyai satu neuron bias bagi setiap neuron output TLU.</li>
<li>Fungsi φ ialah fungsi <em>activation</em>. TLU menggunakan fungsi step sebagai fungsi <em>activation</em>nya (kita akan membincangkan fungsi activation lain pada artikel yang seterusnya).</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Bagaimanakah proses training dilakukan oleh perceptron?</p>
<p>Konsepnya adalah sama dengan yang telah saya jelaskan pada artikel yang <a href="https://megatazm.github.io/MyDLBlog/deep%20learning/2020/12/23/AI-ML-DL.html">sebelum ini</a>. Seperti contoh, setiap data instance atau sample itu dilabelkan dengan nilai target seperti contoh, Hujan(1) &amp; Tidak(0),
<br>
<br>
<img src="https://latex.codecogs.com/gif.download?%5Clarge%20%5C%7Bx_%7B1%7D%2Cx_%7B2%7D%2Ctarget%5C%7D%5Crightarrow%20%5C%7B32%2C25%2C1%5C%7D%2C%5C%7B31%2C24%2C1%5C%7D%5Ccdots%20%5C%7B50%2C45%2C0%5C%7D%2C%5C%7B51%2C44%2C0%5C%7D" alt="equation">
<br>
Dalam proses training, hanya satu instance atau sample di sumbat masuk pada satu masa. Namun hanya input fitur <strong><em>x</em></strong> (tanpa nilai target) sahaja yang akan disambungkan pada neuron TLU. Manakala nilai target daripada instance itu akan digunakan untuk mengukur error (nilai ramalan - target label). Output TLU neuron menghasilkan (melalui fungsi step) nilai ramalan sama ada <strong><em>1</em></strong> atau <strong><em>0</em></strong> berdasarkan pada variasi nilai weight <strong><em>w</em></strong>. Pada setiap kali (melalui proses iteration) neuron output itu menghasilkan ramalan yang salah, ia akan mempertingkatkan nilai weight <strong><em>w</em></strong> antara neuron input dan TLU tersebut sehingga ramalannya menjadi betul. Iaitu pada setiap iteration, nilai weight <strong><em>w</em></strong> akan diupdatekan (lihat rumus di bawah) mengikut error sama ada ke arah 1 atau 0.
<br>
<br>
<img src="https://latex.codecogs.com/gif.download?%5Clarge%20w_%7Bij%7D%3Dw_%7Bij%7D+%5CDelta%20w_%7Bij%7D" alt="equation">
<br>
Tetapi macam mana nak dikaitkan nilai 
<br>
<br>
<img src="https://latex.codecogs.com/gif.download?%5Clarge%20%5CDelta%20w_%7Bij%7D" alt="equation">
<br>
dengan nilai error (ramalan - target label)? Secara logiknya adalah untuk mengurangkan weight <strong><em>w</em></strong> jika neuron diaktifkan (1) walaupun ia sepatutnya tidak (0) diaktifkan, (ramalan: y = 1 dan target: t = 0). Sebaliknya dengan meningkatkan weight <strong><em>w</em></strong> jika neuron tidak (0) diaktifkan walaupun seharusnya diaktifkan (1), (ramalan: y = 0 dan target: t = 1).</p>
<p>Oleh itu kita boleh gantikan dengan
<br>
<br>
<img src="https://latex.codecogs.com/gif.download?%5Clarge%20%5CDelta%20w_%7Bij%7D%3D-%28y-t%29" alt="equation">
<br>
Iaitu, jika sepatutnya aktif (1):</p>
<p><br></p>
<blockquote>
<p><strong><em>Δw(i,j)=-(0–1)=1 (tingkatkan weight w)</em></strong> 
<br></p>
</blockquote>
<p>dan jika tidak sepatutnya aktif (0):<br>&gt;<strong><em>Δw(i,j)=-(1–0)=-1 (kurangkan weight w)</em></strong>
<br></p>
<p>Kita masih lagi satu masalah, bagaimana kalau nilai input adalah negative?</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span> 
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Perceptron</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span> <span class="c1"># load iris data</span>
<span class="nb">print</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)]</span> <span class="c1"># petal length, petal width y = (iris.target == 0).astype(np.int) # Iris setosa?</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span> <span class="c1"># Iris setosa?</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">per_clf</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">()</span>
<span class="n">per_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">per_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
[[1.4 0.2]
 [1.4 0.2]
 [1.3 0.2]
 [1.5 0.2]
 [1.4 0.2]
 [1.7 0.4]
 [1.4 0.3]
 [1.5 0.2]
 [1.4 0.2]
 [1.5 0.1]
 [1.5 0.2]
 [1.6 0.2]
 [1.4 0.1]
 [1.1 0.1]
 [1.2 0.2]
 [1.5 0.4]
 [1.3 0.4]
 [1.4 0.3]
 [1.7 0.3]
 [1.5 0.3]
 [1.7 0.2]
 [1.5 0.4]
 [1.  0.2]
 [1.7 0.5]
 [1.9 0.2]
 [1.6 0.2]
 [1.6 0.4]
 [1.5 0.2]
 [1.4 0.2]
 [1.6 0.2]
 [1.6 0.2]
 [1.5 0.4]
 [1.5 0.1]
 [1.4 0.2]
 [1.5 0.2]
 [1.2 0.2]
 [1.3 0.2]
 [1.4 0.1]
 [1.3 0.2]
 [1.5 0.2]
 [1.3 0.3]
 [1.3 0.3]
 [1.3 0.2]
 [1.6 0.6]
 [1.9 0.4]
 [1.4 0.3]
 [1.6 0.2]
 [1.4 0.2]
 [1.5 0.2]
 [1.4 0.2]
 [4.7 1.4]
 [4.5 1.5]
 [4.9 1.5]
 [4.  1.3]
 [4.6 1.5]
 [4.5 1.3]
 [4.7 1.6]
 [3.3 1. ]
 [4.6 1.3]
 [3.9 1.4]
 [3.5 1. ]
 [4.2 1.5]
 [4.  1. ]
 [4.7 1.4]
 [3.6 1.3]
 [4.4 1.4]
 [4.5 1.5]
 [4.1 1. ]
 [4.5 1.5]
 [3.9 1.1]
 [4.8 1.8]
 [4.  1.3]
 [4.9 1.5]
 [4.7 1.2]
 [4.3 1.3]
 [4.4 1.4]
 [4.8 1.4]
 [5.  1.7]
 [4.5 1.5]
 [3.5 1. ]
 [3.8 1.1]
 [3.7 1. ]
 [3.9 1.2]
 [5.1 1.6]
 [4.5 1.5]
 [4.5 1.6]
 [4.7 1.5]
 [4.4 1.3]
 [4.1 1.3]
 [4.  1.3]
 [4.4 1.2]
 [4.6 1.4]
 [4.  1.2]
 [3.3 1. ]
 [4.2 1.3]
 [4.2 1.2]
 [4.2 1.3]
 [4.3 1.3]
 [3.  1.1]
 [4.1 1.3]
 [6.  2.5]
 [5.1 1.9]
 [5.9 2.1]
 [5.6 1.8]
 [5.8 2.2]
 [6.6 2.1]
 [4.5 1.7]
 [6.3 1.8]
 [5.8 1.8]
 [6.1 2.5]
 [5.1 2. ]
 [5.3 1.9]
 [5.5 2.1]
 [5.  2. ]
 [5.1 2.4]
 [5.3 2.3]
 [5.5 1.8]
 [6.7 2.2]
 [6.9 2.3]
 [5.  1.5]
 [5.7 2.3]
 [4.9 2. ]
 [6.7 2. ]
 [4.9 1.8]
 [5.7 2.1]
 [6.  1.8]
 [4.8 1.8]
 [4.9 1.8]
 [5.6 2.1]
 [5.8 1.6]
 [6.1 1.9]
 [6.4 2. ]
 [5.6 2.2]
 [5.1 1.5]
 [5.6 1.4]
 [6.1 2.3]
 [5.6 2.4]
 [5.5 1.8]
 [4.8 1.8]
 [5.4 2.1]
 [5.6 2.4]
 [5.1 2.3]
 [5.1 1.9]
 [5.9 2.3]
 [5.7 2.5]
 [5.2 2.3]
 [5.  1.9]
 [5.2 2. ]
 [5.4 2.3]
 [5.1 1.8]] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1]
[1]
</pre>
</div>
</div>

</div>
</div>

</div>
    

</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="megatazm/MyDLBlog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/MyDLBlog/deep%20learning/2020/12/29/basic-dl.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/MyDLBlog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/MyDLBlog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/MyDLBlog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/MyDLBlog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/MyDLBlog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
